{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "\n",
    "The dataset for this challenge has been obtained from the European Social Survey. Our objective for this challenge is to determine what variables we can use to predict if a person has a partner or not, and how significant each variable is for predicting the outcome.\n",
    "\n",
    "# Exercise\n",
    "\n",
    "From our initial decision tree, we were able to predict whether someone has a partner or not with an error rate of 6.258% for false positives, and 18.528% for false negatives. The challenge here is to reduce those error rates through modifying the features and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def booster (X, y, iterations, loss, depth):\n",
    "    # Create training and test sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2)\n",
    "\n",
    "    # We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "    params = {'n_estimators': iterations,\n",
    "              'max_depth': depth,\n",
    "              'loss': loss}\n",
    "\n",
    "    # Initialize and fit the model.\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "\n",
    "    # Accuracy tables.\n",
    "    table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "    table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "    train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "    test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "    print((\n",
    "        'Training set accuracy:\\n'\n",
    "        'Percent Type I errors: {}\\n'\n",
    "        'Percent Type II errors: {}\\n\\n'\n",
    "        'Test set accuracy:\\n'\n",
    "        'Percent Type I errors: {}\\n'\n",
    "        'Percent Type II errors: {}'\n",
    "    ).format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.047107564830443455\n",
      "Percent Type II errors: 0.17247199631732393\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06441717791411043\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X1 = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X1 = pd.concat([X1, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "booster(X1, y, 500, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04741445450360595\n",
      "Percent Type II errors: 0.1755408930489489\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05337423312883435\n",
      "Percent Type II errors: 0.20552147239263804\n"
     ]
    }
   ],
   "source": [
    "# Changed loss type - less accurate\n",
    "booster(X1, y, 500, 'exponential', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04864201319625595\n",
      "Percent Type II errors: 0.1704772134417677\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06012269938650307\n",
      "Percent Type II errors: 0.17975460122699385\n"
     ]
    }
   ],
   "source": [
    "# Doubled number of iterations + more accurate\n",
    "booster(X1, y, 1000, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.03299063986496854\n",
      "Percent Type II errors: 0.13180911462329292\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.08159509202453988\n",
      "Percent Type II errors: 0.18834355828220858\n"
     ]
    }
   ],
   "source": [
    "# Significantly higher number of iterations + less accurate\n",
    "booster(X1, y, 10000, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.01764615620684364\n",
      "Percent Type II errors: 0.11048028233849931\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06932515337423313\n",
      "Percent Type II errors: 0.18650306748466258\n"
     ]
    }
   ],
   "source": [
    "# Doubled tree depth + less accurate\n",
    "booster(X1, y, 500, 'deviance', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X1 ** 2\n",
    "X3 = np.sqrt(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04618689581095596\n",
      "Percent Type II errors: 0.17170477213441768\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0705521472392638\n",
      "Percent Type II errors: 0.19877300613496932\n"
     ]
    }
   ],
   "source": [
    "booster(X1 + X2, y, 500, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.047261009667024706\n",
      "Percent Type II errors: 0.17569433788553016\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05889570552147239\n",
      "Percent Type II errors: 0.18466257668711655\n"
     ]
    }
   ],
   "source": [
    "booster(X3, y, 500, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04894890286941844\n",
      "Percent Type II errors: 0.1758477827221114\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.050920245398773004\n",
      "Percent Type II errors: 0.18282208588957055\n"
     ]
    }
   ],
   "source": [
    "booster(X1, y, 500, 'deviance', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04940923737916219\n",
      "Percent Type II errors: 0.17692189657818014\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06503067484662577\n",
      "Percent Type II errors: 0.18404907975460122\n"
     ]
    }
   ],
   "source": [
    "booster(X1, y, 500, 'exponential', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Through modifying our parameters for modelling, there seem to be a few ways to improve our model yet. Modifying the number of iterations in our test seems to indicate that while doubling the number of iterations for our model will improve the accuracy, there can be too much of a good thing as performing the model 20 times will lead to overfitting. Manipulating our features by taking the square root of them also seems to be a better predictor of our data. \n",
    "\n",
    "On the other hand, modifying the loss type from deviance to exponential seems to have a mixed result as our rate of false positives will decrease, but our rate of false negatives increases by a similar amount. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
