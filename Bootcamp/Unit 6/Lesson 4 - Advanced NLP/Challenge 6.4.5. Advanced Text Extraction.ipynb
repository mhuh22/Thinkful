{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Topic extraction on new data\n",
    "\n",
    "Take the well-known [20 newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset and use each of the methods on it.  Your goal is to determine which method, if any, best reproduces the topics represented by the newsgroups.  Write up a report where you evaluate each method in light of the 'ground truth'- the known source of each newsgroup post.  Which works best, and why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 20 newsgroups \n",
    "twenty_newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "# Print first entry in dataset\n",
    "print(\"\\n\".join(twenty_newsgroups.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the tf-idf matrix\n",
    "(term frequency - inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating tf-idf matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "twenty_newsgroups_tfidf = vectorizer.fit_transform(twenty_newsgroups.data)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=20\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the 3 topic extraction models\n",
    "\n",
    "### LSA (Latent Semantic Analysis)\n",
    "\n",
    "Returns clusters of terms that reflect a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Parameters for LSA\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Time and run LSA model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_lsa = lsa.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_lsa = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for LSA\n",
    "components_lsa = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_lsa, terms)\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "Estimates the probability that a topic will be in a document, and the probability that a word will be in a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhuh22\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mhuh22\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Parameters for LDA\n",
    "lda = LDA(n_topics=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "# Time and run LDA model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_lda = lda.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_lda = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for LDA\n",
    "components_lda = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_lda, terms)\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNMF (Non-negative Matrix Factorization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Parameters for NNMF\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "\n",
    "# Time and run NNMF model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_nmf = nmf.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_nnmf = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for NNMF\n",
    "components_nmf = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_nmf, terms)\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "            LSA             LDA         NNMF\n",
      "0    like 87.76   armenian 2.92    just 3.23\n",
      "0     don 86.51  armenians 2.22     don 3.22\n",
      "0    just 85.16    turkish 1.92   think 2.61\n",
      "0    know 80.12     people 1.75    like 2.49\n",
      "0  people 79.19       like 1.54  people 2.08\n",
      "0   think 74.78       know 1.53    know 1.99\n",
      "0    does 62.14       just 1.48    good 1.63\n",
      "0    good 62.07        don 1.43      ve 1.54\n",
      "0    time 61.16       does 1.37    time 1.51\n",
      "0     use 59.87     thanks 1.29     say 1.43\n",
      "Topic 1:\n",
      "              LSA          LDA           NNMF\n",
      "1    windows 32.2      ax 5.25        use 2.2\n",
      "1    thanks 30.84    know 1.24       mac 1.42\n",
      "1      card 20.73    just 1.15  software 1.26\n",
      "1     drive 19.48    like 1.12     apple 0.97\n",
      "1       dos 17.02     don 1.08      like 0.97\n",
      "1       use 16.74    does 1.06      need 0.88\n",
      "1      file 16.07  thanks 1.05   problem 0.87\n",
      "1  software 16.06   think 0.98     modem 0.86\n",
      "1      mail 15.78     use 0.91      used 0.84\n",
      "1        pc 15.53  people 0.87    memory 0.82\n",
      "Topic 2:\n",
      "                LSA          LDA             NNMF\n",
      "2          god 44.0    just 1.18         god 7.77\n",
      "2       jesus 18.92    know 1.15       jesus 3.35\n",
      "2     windows 14.38    like 1.11       bible 2.03\n",
      "2       bible 12.47     don 1.04     believe 1.85\n",
      "2      thanks 11.96    does 1.02      people 1.77\n",
      "2        does 11.75   thanks 1.0        faith 1.6\n",
      "2  christians 10.31    think 0.9      christ 1.55\n",
      "2   christian 10.04  people 0.85   christian 1.52\n",
      "2        faith 9.55      use 0.8  christians 1.49\n",
      "2       christ 9.29    good 0.75         say 1.37\n",
      "Topic 3:\n",
      "               LSA          LDA             NNMF\n",
      "3        edu 17.67    know 1.16         geb 3.07\n",
      "3       pitt 11.64    just 1.16         dsl 3.04\n",
      "3     gordon 11.16    like 1.12       n3jxp 3.04\n",
      "3  surrender 11.14     does 1.1    chastity 3.04\n",
      "3        geb 10.96     don 1.07       cadre 3.04\n",
      "3      banks 10.86  thanks 1.03        pitt 3.03\n",
      "3       soon 10.83   think 0.95    shameful 3.03\n",
      "3   shameful 10.73     use 0.92   intellect 3.01\n",
      "3       cadre 10.6   people 0.9  skepticism 3.01\n",
      "3   chastity 10.57     good 0.8   surrender 2.98\n",
      "Topic 4:\n",
      "                LSA          LDA             NNMF\n",
      "4         key 23.09    know 1.15         key 4.97\n",
      "4  encryption 14.01    just 1.14        chip 3.09\n",
      "4        chip 13.95     like 1.1  encryption 2.66\n",
      "4      clipper 12.0     don 1.04     clipper 2.34\n",
      "4   government 11.4    does 1.02        keys 2.09\n",
      "4          use 11.3   thanks 1.0  government 1.55\n",
      "4        keys 10.38    think 0.9      escrow 1.46\n",
      "4          law 8.32  people 0.85   algorithm 1.25\n",
      "4       public 8.31      use 0.8         use 1.17\n",
      "4         file 8.24    good 0.75         law 1.09\n",
      "Topic 5:\n",
      "               LSA            LDA             NNMF\n",
      "5      drive 31.88    like 123.41       drive 7.08\n",
      "5       scsi 17.33    just 123.15        disk 2.21\n",
      "5       hard 10.69     don 120.57        hard 2.03\n",
      "5     drives 10.46    know 119.89      drives 1.96\n",
      "5         disk 9.9  people 110.33      floppy 1.55\n",
      "5  controller 9.75    think 103.3        scsi 1.52\n",
      "5         ide 9.45    does 100.73         ide 1.27\n",
      "5        chip 8.62       use 90.3  controller 1.15\n",
      "5        card 7.62     good 87.49        boot 0.84\n",
      "5         bus 7.59     time 85.53     problem 0.83\n",
      "Topic 6:\n",
      "          LSA          LDA          NNMF\n",
      "6   god 20.59    know 1.17     game 2.76\n",
      "6  game 15.76    just 1.14     team 2.44\n",
      "6   key 13.26    like 1.11    games 1.85\n",
      "6  games 13.2     don 1.04     year 1.68\n",
      "6  team 13.02    does 1.04  players 1.36\n",
      "6   mail 8.91  thanks 1.01   season 1.27\n",
      "6   chip 8.71    think 0.9      play 1.2\n",
      "6  jesus 8.32  people 0.86   hockey 1.18\n",
      "6     00 8.23      use 0.8      win 0.99\n",
      "6    edu 8.21    good 0.76   league 0.92\n",
      "Topic 7:\n",
      "             LSA          LDA              NNMF\n",
      "7   thanks 34.86    know 1.15       thanks 7.39\n",
      "7     know 25.33    just 1.14      advance 2.89\n",
      "7     does 19.69     like 1.1           hi 2.47\n",
      "7     mail 17.79     don 1.04         mail 2.23\n",
      "7  advance 11.92    does 1.02         know 2.07\n",
      "7  looking 11.19   thanks 1.0      looking 1.93\n",
      "7     info 10.58    think 0.9         does 1.72\n",
      "7        hi 9.81  people 0.85         help 1.71\n",
      "7     email 9.27      use 0.8         info 1.62\n",
      "7   address 8.93    good 0.75  appreciated 1.47\n",
      "Topic 8:\n",
      "                LSA          LDA         NNMF\n",
      "8           00 13.4    know 1.15     car 3.85\n",
      "8         edu 12.32    just 1.14     bike 1.1\n",
      "8         new 12.05     like 1.1      new 0.9\n",
      "8       space 10.45     don 1.04    cars 0.87\n",
      "8        sale 10.36    does 1.02    good 0.86\n",
      "8           10 8.34   thanks 1.0    like 0.79\n",
      "8       israel 8.22    think 0.9  engine 0.69\n",
      "8        email 8.19  people 0.86    just 0.65\n",
      "8         mail 8.02      use 0.8    miles 0.6\n",
      "8  information 7.19    good 0.75  dealer 0.56\n",
      "Topic 9:\n",
      "             LSA          LDA          NNMF\n",
      "9     card 22.45    just 1.17     card 5.58\n",
      "9    video 12.17    know 1.17    video 2.68\n",
      "9      car 11.81    like 1.13  monitor 1.99\n",
      "9  monitor 11.43     don 1.08  drivers 1.56\n",
      "9      just 7.91    does 1.06      vga 1.51\n",
      "9     price 7.85   thanks 1.0      bus 1.39\n",
      "9   drivers 7.06    think 0.9    cards 1.33\n",
      "9       new 6.94  people 0.85  windows 1.26\n",
      "9      bike 6.77     use 0.82    driver 1.2\n",
      "9      sale 6.58    good 0.76    color 1.17\n",
      "Topic 10:\n",
      "                LSA          LDA             NNMF\n",
      "10       card 17.36    know 1.34      people 3.72\n",
      "10       does 11.61    like 1.32  government 1.91\n",
      "10     israel 10.59    just 1.27         gun 1.36\n",
      "10       game 10.25      ax 1.17     armenian 1.3\n",
      "10       video 8.29    does 1.17         don 1.22\n",
      "10        team 8.25     don 1.16       right 1.13\n",
      "10       people 7.7  thanks 1.16         law 1.09\n",
      "10        jews 7.44   think 1.05       think 1.08\n",
      "10      israeli 6.8     use 0.96   armenians 1.05\n",
      "10  government 6.79    good 0.95     turkish 0.96\n",
      "Topic 11:\n",
      "              LSA          LDA          NNMF\n",
      "11  windows 16.89     edu 1.26   windows 5.4\n",
      "11      car 16.01    just 1.19      dos 2.58\n",
      "11        00 9.16    know 1.18       ms 1.01\n",
      "11       dos 8.93    like 1.11     file 1.01\n",
      "11    thanks 7.75     don 1.07  version 0.92\n",
      "11     drive 6.95    does 1.03     files 0.9\n",
      "11      bike 6.28  thanks 1.01  running 0.89\n",
      "11       new 5.79   think 0.91   thanks 0.83\n",
      "11      sale 5.41  people 0.88       use 0.8\n",
      "11     price 5.38    good 0.83  drivers 0.77\n",
      "Topic 12:\n",
      "              LSA          LDA              NNMF\n",
      "12   window 17.59    know 1.22       window 5.05\n",
      "12   israel 12.32    like 1.15        motif 1.42\n",
      "12  problem 10.85    just 1.14       server 1.36\n",
      "12     does 10.73    does 1.09  application 1.28\n",
      "12   israeli 7.03     don 1.09      problem 1.22\n",
      "12      jews 6.81  thanks 1.01      manager 1.18\n",
      "12       car 6.41   think 0.94       windows 1.1\n",
      "12     motif 6.15  people 0.87      program 1.04\n",
      "12    server 5.83     use 0.83      display 0.99\n",
      "12     using 5.66    good 0.78        using 0.97\n",
      "Topic 13:\n",
      "              LSA          LDA          NNMF\n",
      "13      car 17.58    know 1.23    space 4.17\n",
      "13    space 11.96    just 1.22     nasa 1.69\n",
      "13      year 6.36    like 1.18  shuttle 0.87\n",
      "13      said 6.02      don 1.1   launch 0.85\n",
      "13      does 5.84    does 1.05  program 0.76\n",
      "13      time 5.73  thanks 1.04    orbit 0.71\n",
      "13      cars 5.39   think 0.93     data 0.69\n",
      "13     years 5.38      use 0.9     moon 0.68\n",
      "13  armenian 5.17  people 0.89    earth 0.67\n",
      "13       bus 5.15    good 0.79     like 0.63\n",
      "Topic 14:\n",
      "             LSA          LDA          NNMF\n",
      "14  israel 15.49    know 1.16     does 6.16\n",
      "14     car 10.73    just 1.15     know 5.14\n",
      "14     does 9.47     like 1.1   thanks 1.68\n",
      "14  israeli 8.52     don 1.04      don 1.47\n",
      "14     jews 6.66    does 1.03     like 1.38\n",
      "14  deleted 6.18   thanks 1.0  anybody 1.37\n",
      "14     good 5.61   think 0.91     just 1.14\n",
      "14     arab 5.35  people 0.86   people 1.06\n",
      "14     space 5.1      use 0.8      use 0.93\n",
      "14    stuff 4.54    good 0.75      god 0.93\n",
      "Topic 15:\n",
      "                LSA          LDA           NNMF\n",
      "15       does 21.39     just 1.2    israel 5.03\n",
      "15        gun 10.53    know 1.17   israeli 2.67\n",
      "15  government 8.59    like 1.13      jews 2.27\n",
      "15         use 8.11    does 1.09      arab 1.71\n",
      "15         law 8.08     don 1.07     arabs 1.17\n",
      "15      window 7.49  thanks 1.01    jewish 1.04\n",
      "15        know 7.12   think 0.92   lebanese 1.0\n",
      "15         car 6.56  people 0.87   lebanon 0.97\n",
      "15        guns 5.65     use 0.86     peace 0.96\n",
      "15        laws 5.17    good 0.76  israelis 0.86\n",
      "Topic 16:\n",
      "             LSA          LDA           NNMF\n",
      "16     know 26.3    just 1.26        00 5.22\n",
      "16    does 15.65    like 1.23         10 1.4\n",
      "16       00 14.1    know 1.22      sale 1.27\n",
      "16     don 10.76    does 1.11       new 1.18\n",
      "16    space 8.26      don 1.1        50 1.16\n",
      "16      just 6.1  thanks 1.03        20 1.06\n",
      "16       10 4.93    think 1.0  shipping 1.02\n",
      "16  deleted 4.87  people 0.92        15 0.98\n",
      "16       key 4.8     use 0.91     price 0.98\n",
      "16     file 3.68    good 0.82        25 0.87\n",
      "Topic 17:\n",
      "          LSA           LDA             NNMF\n",
      "17  edu 18.55     know 1.26        scsi 6.42\n",
      "17  com 16.09     just 1.18         ide 2.04\n",
      "17   car 8.46     like 1.16       drive 1.75\n",
      "17    don 8.0      don 1.12  controller 1.71\n",
      "17  know 7.95    thanks 1.1         bus 1.56\n",
      "17   ftp 6.97     does 1.08         isa 1.05\n",
      "17    ve 6.67  catbyte 1.05         bit 1.02\n",
      "17  said 6.18  dtmedin 1.05          pc 0.92\n",
      "17  file 6.09     ingr 1.03      drives 0.72\n",
      "17   gun 6.08      b30 1.03         mac 0.71\n",
      "Topic 18:\n",
      "             LSA          LDA            NNMF\n",
      "18  thanks 11.91    know 1.18       file 4.95\n",
      "18     scsi 9.55    just 1.16      files 3.37\n",
      "18    think 8.97    like 1.14    program 1.82\n",
      "18       00 7.25     don 1.07     windows 1.8\n",
      "18  windows 6.66    does 1.06        ftp 1.66\n",
      "18      bus 6.07  thanks 1.04        use 1.09\n",
      "18     time 5.75   think 0.94  directory 1.04\n",
      "18   memory 5.61  people 0.89     format 1.02\n",
      "18   window 5.55     use 0.82      image 0.99\n",
      "18       bit 5.5    good 0.77        dos 0.96\n",
      "Topic 19:\n",
      "            LSA          LDA          NNMF\n",
      "19   just 24.97    know 1.15      edu 3.04\n",
      "19    file 10.9    just 1.15     mail 2.52\n",
      "19  thanks 8.73     like 1.1      com 2.03\n",
      "19    does 8.59     don 1.04     list 1.83\n",
      "19     gun 7.38    does 1.02  address 1.56\n",
      "19  memory 6.59   thanks 1.0     send 1.45\n",
      "19     bit 6.44    think 0.9    email 1.39\n",
      "19      00 5.23  people 0.86   thanks 1.09\n",
      "19     law 4.96      use 0.8      like 1.0\n",
      "19     did 4.85    good 0.75     know 0.95\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA (Latent Semantic Analysis) runtime: 5.2903826422031175 \n",
      "\n",
      "LDA (Latent Dirichlet Allocation) runtime: 1516.9622960999652 \n",
      "\n",
      "NNMF (Non-Negative Matrix Factorization) runtime: 38.7235158709434 \n"
     ]
    }
   ],
   "source": [
    "print('LSA (Latent Semantic Analysis) runtime: {} \\n'.format(elapsed_lsa))\n",
    "print('LDA (Latent Dirichlet Allocation) runtime: {} \\n'.format(elapsed_lda))\n",
    "print('NNMF (Non-Negative Matrix Factorization) runtime: {} '.format(elapsed_nnmf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
