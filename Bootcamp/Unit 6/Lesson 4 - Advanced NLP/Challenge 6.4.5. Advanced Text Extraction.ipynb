{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Topic extraction on new data\n",
    "\n",
    "Take the well-known [20 newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset and use each of the methods on it.  Your goal is to determine which method, if any, best reproduces the topics represented by the newsgroups.  Write up a report where you evaluate each method in light of the 'ground truth'- the known source of each newsgroup post.  Which works best, and why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 20 newsgroups \n",
    "twenty_newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "# Print first entry in dataset\n",
    "print(\"\\n\".join(twenty_newsgroups.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the tf-idf matrix\n",
    "(term frequency - inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating tf-idf matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "twenty_newsgroups_tfidf = vectorizer.fit_transform(twenty_newsgroups.data)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=20\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the 3 topic extraction models\n",
    "\n",
    "### LSA (Latent Semantic Analysis)\n",
    "\n",
    "Returns clusters of terms that reflect a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Parameters for LSA\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Time and run LSA model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_lsa = lsa.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_lsa = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for LSA\n",
    "components_lsa = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_lsa, terms)\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "Estimates the probability that a topic will be in a document, and the probability that a word will be in a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nu\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\nu\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Parameters for LDA\n",
    "lda = LDA(n_topics=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "# Time and run LDA model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_lda = lda.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_lda = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for LDA\n",
    "components_lda = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_lda, terms)\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNMF (Non-negative Matrix Factorization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Parameters for NNMF\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "\n",
    "# Time and run NNMF model\n",
    "start_time = timeit.default_timer()\n",
    "twenty_newsgroups_nmf = nmf.fit_transform(twenty_newsgroups_tfidf)\n",
    "elapsed_nnmf = timeit.default_timer() - start_time\n",
    "\n",
    "# Extract most common words for NNMF\n",
    "components_nmf = word_topic(twenty_newsgroups_tfidf, twenty_newsgroups_nmf, terms)\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "            LSA             LDA         NNMF\n",
      "0    like 87.91   armenian 2.92    just 3.23\n",
      "0     don 86.96  armenians 2.22     don 3.22\n",
      "0    just 86.08    turkish 1.92   think 2.61\n",
      "0    know 81.03     people 1.75    like 2.49\n",
      "0  people 79.09       like 1.54  people 2.08\n",
      "0   think 74.01       know 1.53    know 1.99\n",
      "0    good 62.16       just 1.48    good 1.63\n",
      "0    does 62.14        don 1.43      ve 1.54\n",
      "0    time 61.26       does 1.37    time 1.51\n",
      "0     use 59.54     thanks 1.29     say 1.43\n",
      "Topic 1:\n",
      "              LSA          LDA           NNMF\n",
      "1   windows 32.35      ax 5.25        use 2.2\n",
      "1    thanks 31.23    know 1.24       mac 1.42\n",
      "1       card 20.7    just 1.15  software 1.26\n",
      "1     drive 19.33    like 1.12     apple 0.97\n",
      "1       dos 17.17     don 1.08      like 0.97\n",
      "1       use 16.52    does 1.06      need 0.88\n",
      "1      file 16.38  thanks 1.05   problem 0.87\n",
      "1  software 16.06   think 0.98     modem 0.86\n",
      "1      mail 15.75     use 0.91      used 0.84\n",
      "1        pc 15.46  people 0.87    memory 0.82\n",
      "Topic 2:\n",
      "                LSA          LDA             NNMF\n",
      "2         god 43.78    just 1.18         god 7.77\n",
      "2       jesus 18.61    know 1.15       jesus 3.35\n",
      "2     windows 14.44    like 1.11       bible 2.03\n",
      "2       bible 12.47     don 1.04     believe 1.85\n",
      "2      thanks 12.07    does 1.02      people 1.77\n",
      "2        does 11.64   thanks 1.0        faith 1.6\n",
      "2  christians 10.22    think 0.9      christ 1.55\n",
      "2    christian 9.91  people 0.85   christian 1.52\n",
      "2        faith 9.44      use 0.8  christians 1.49\n",
      "2       christ 9.23    good 0.75         say 1.37\n",
      "Topic 3:\n",
      "               LSA          LDA             NNMF\n",
      "3        edu 17.89    know 1.16         geb 3.07\n",
      "3       pitt 11.64    just 1.16         dsl 3.04\n",
      "3     gordon 11.16    like 1.12       n3jxp 3.04\n",
      "3  surrender 11.15     does 1.1    chastity 3.04\n",
      "3        geb 10.96     don 1.07       cadre 3.04\n",
      "3      banks 10.88  thanks 1.03        pitt 3.03\n",
      "3       soon 10.84   think 0.95    shameful 3.03\n",
      "3   shameful 10.73     use 0.92   intellect 3.01\n",
      "3       cadre 10.6   people 0.9  skepticism 3.01\n",
      "3      n3jxp 10.58     good 0.8   surrender 2.98\n",
      "Topic 4:\n",
      "                LSA          LDA             NNMF\n",
      "4         key 23.24    know 1.15         key 4.97\n",
      "4  encryption 14.05    just 1.14        chip 3.09\n",
      "4        chip 14.02     like 1.1  encryption 2.66\n",
      "4     clipper 12.03     don 1.04     clipper 2.34\n",
      "4  government 11.69    does 1.02        keys 2.09\n",
      "4          use 11.2   thanks 1.0  government 1.55\n",
      "4        keys 10.46    think 0.9      escrow 1.46\n",
      "4         file 8.56  people 0.85   algorithm 1.25\n",
      "4          law 8.47      use 0.8         use 1.17\n",
      "4        public 8.4    good 0.75         law 1.09\n",
      "Topic 5:\n",
      "               LSA            LDA             NNMF\n",
      "5      drive 31.56    like 123.41       drive 7.08\n",
      "5       scsi 16.67    just 123.15        disk 2.21\n",
      "5        hard 10.6     don 120.57        hard 2.03\n",
      "5     drives 10.39    know 119.89      drives 1.96\n",
      "5        disk 9.78  people 110.33      floppy 1.55\n",
      "5  controller 9.75    think 103.3        scsi 1.52\n",
      "5         ide 9.45    does 100.73         ide 1.27\n",
      "5        chip 8.52       use 90.3  controller 1.15\n",
      "5         bus 7.88     good 87.49        boot 0.84\n",
      "5        card 7.69     time 85.53     problem 0.83\n",
      "Topic 6:\n",
      "           LSA          LDA          NNMF\n",
      "6    god 20.42    know 1.17     game 2.76\n",
      "6   game 15.29    just 1.14     team 2.44\n",
      "6    key 13.36    like 1.11    games 1.85\n",
      "6  games 12.88     don 1.04     year 1.68\n",
      "6    team 12.8    does 1.04  players 1.36\n",
      "6    mail 9.19  thanks 1.01   season 1.27\n",
      "6    chip 8.81    think 0.9      play 1.2\n",
      "6      00 8.72  people 0.86   hockey 1.18\n",
      "6     edu 8.06      use 0.8      win 0.99\n",
      "6    year 8.04    good 0.76   league 0.92\n",
      "Topic 7:\n",
      "             LSA          LDA              NNMF\n",
      "7   thanks 35.66    know 1.15       thanks 7.39\n",
      "7     know 25.33    just 1.14      advance 2.89\n",
      "7     does 20.02     like 1.1           hi 2.47\n",
      "7     mail 17.23     don 1.04         mail 2.23\n",
      "7  advance 12.15    does 1.02         know 2.07\n",
      "7  looking 11.27   thanks 1.0      looking 1.93\n",
      "7     info 10.63    think 0.9         does 1.72\n",
      "7        hi 10.0  people 0.85         help 1.71\n",
      "7     email 9.33      use 0.8         info 1.62\n",
      "7   address 8.59    good 0.75  appreciated 1.47\n",
      "Topic 8:\n",
      "             LSA          LDA         NNMF\n",
      "8       00 13.29    know 1.15     car 3.85\n",
      "8      edu 12.72    just 1.14     bike 1.1\n",
      "8      new 12.38     like 1.1      new 0.9\n",
      "8     sale 10.66     don 1.04    cars 0.87\n",
      "8    space 10.12    does 1.02    good 0.86\n",
      "8    israel 8.33   thanks 1.0    like 0.79\n",
      "8        10 8.26    think 0.9  engine 0.69\n",
      "8      mail 8.14  people 0.86    just 0.65\n",
      "8      email 8.1      use 0.8    miles 0.6\n",
      "8  shipping 7.26    good 0.75  dealer 0.56\n",
      "Topic 9:\n",
      "             LSA          LDA          NNMF\n",
      "9     card 22.18    just 1.17     card 5.58\n",
      "9      car 13.32    know 1.17    video 2.68\n",
      "9    video 11.71    like 1.13  monitor 1.99\n",
      "9  monitor 10.47     don 1.08  drivers 1.56\n",
      "9       just 9.1    does 1.06      vga 1.51\n",
      "9     price 7.72   thanks 1.0      bus 1.39\n",
      "9   drivers 6.81    think 0.9    cards 1.33\n",
      "9      bike 6.66  people 0.85  windows 1.26\n",
      "9     cards 6.31     use 0.82    driver 1.2\n",
      "9        00 5.98    good 0.76    color 1.17\n",
      "Topic 10:\n",
      "             LSA          LDA             NNMF\n",
      "10     card 17.2    know 1.34      people 3.72\n",
      "10    does 11.86    like 1.32  government 1.91\n",
      "10  israel 10.44    just 1.27         gun 1.36\n",
      "10    game 10.42      ax 1.17     armenian 1.3\n",
      "10     team 8.38    does 1.17         don 1.22\n",
      "10    video 8.27     don 1.16       right 1.13\n",
      "10   people 8.21  thanks 1.16         law 1.09\n",
      "10     jews 7.43   think 1.05       think 1.08\n",
      "10     games 6.9     use 0.96   armenians 1.05\n",
      "10  israeli 6.74    good 0.95     turkish 0.96\n",
      "Topic 11:\n",
      "              LSA          LDA          NNMF\n",
      "11    space 13.13     edu 1.26   windows 5.4\n",
      "11        com 8.7    just 1.19      dos 2.58\n",
      "11      data 7.79    know 1.18       ms 1.01\n",
      "11       edu 7.13    like 1.11     file 1.01\n",
      "11  graphics 7.04     don 1.07  version 0.92\n",
      "11      nasa 6.56    does 1.03     files 0.9\n",
      "11       bit 6.52  thanks 1.01  running 0.89\n",
      "11     think 5.91   think 0.91   thanks 0.83\n",
      "11      time 5.77  people 0.88       use 0.8\n",
      "11       mac 5.42    good 0.83  drivers 0.77\n",
      "Topic 12:\n",
      "              LSA          LDA              NNMF\n",
      "12   window 15.82    know 1.22       window 5.05\n",
      "12   israel 13.52    like 1.15        motif 1.42\n",
      "12  problem 10.13    just 1.14       server 1.36\n",
      "12      does 8.07    does 1.09  application 1.28\n",
      "12   israeli 7.75     don 1.09      problem 1.22\n",
      "12      jews 7.19  thanks 1.01      manager 1.18\n",
      "12       car 6.74   think 0.94       windows 1.1\n",
      "12     motif 6.16  people 0.87      program 1.04\n",
      "12       key 5.54     use 0.83      display 0.99\n",
      "12    server 5.51    good 0.78        using 0.97\n",
      "Topic 13:\n",
      "             LSA          LDA          NNMF\n",
      "13   space 16.06    know 1.23    space 4.17\n",
      "13    does 15.12    just 1.22     nasa 1.69\n",
      "13     car 14.77    like 1.18  shuttle 0.87\n",
      "13     know 7.96      don 1.1   launch 0.85\n",
      "13     nasa 6.04    does 1.05  program 0.76\n",
      "13     file 5.69  thanks 1.04    orbit 0.71\n",
      "13     year 5.67   think 0.93     data 0.69\n",
      "13    years 5.14      use 0.9     moon 0.68\n",
      "13  program 4.66  people 0.89    earth 0.67\n",
      "13    files 4.66    good 0.79     like 0.63\n",
      "Topic 14:\n",
      "            LSA          LDA          NNMF\n",
      "14   does 25.23    know 1.16     does 6.16\n",
      "14   know 10.85    just 1.15     know 5.14\n",
      "14     gun 8.42     like 1.1   thanks 1.68\n",
      "14      00 7.85     don 1.04      don 1.47\n",
      "14     use 6.12    does 1.03     like 1.38\n",
      "14      don 5.8   thanks 1.0  anybody 1.37\n",
      "14   think 5.11   think 0.91     just 1.14\n",
      "14     law 4.91  people 0.86   people 1.06\n",
      "14    make 4.85      use 0.8      use 0.93\n",
      "14  window 4.48    good 0.75      god 0.93\n",
      "Topic 15:\n",
      "             LSA          LDA           NNMF\n",
      "15    know 13.95     just 1.2    israel 5.03\n",
      "15  israel 12.86    know 1.17   israeli 2.67\n",
      "15     don 10.64    like 1.13      jews 2.27\n",
      "15      00 10.27    does 1.09      arab 1.71\n",
      "15    just 10.19     don 1.07     arabs 1.17\n",
      "15    space 7.87  thanks 1.01    jewish 1.04\n",
      "15  israeli 7.02   think 0.92   lebanese 1.0\n",
      "15     does 6.21  people 0.87   lebanon 0.97\n",
      "15     jews 6.02     use 0.86     peace 0.96\n",
      "15    think 5.82    good 0.76  israelis 0.86\n",
      "Topic 16:\n",
      "               LSA          LDA           NNMF\n",
      "16      know 17.48    just 1.26        00 5.22\n",
      "16      does 10.32    like 1.23         10 1.4\n",
      "16        00 10.21    know 1.22      sale 1.27\n",
      "16        don 7.13    does 1.11       new 1.18\n",
      "16   armenian 6.95      don 1.1        50 1.16\n",
      "16  armenians 6.37  thanks 1.03        20 1.06\n",
      "16      window 6.2    think 1.0  shipping 1.02\n",
      "16    turkish 5.68  people 0.92        15 0.98\n",
      "16         10 5.65     use 0.91     price 0.98\n",
      "16       just 5.21    good 0.82        25 0.87\n",
      "Topic 17:\n",
      "           LSA           LDA             NNMF\n",
      "17  scsi 12.93     know 1.26        scsi 6.42\n",
      "17   car 12.14     just 1.18         ide 2.04\n",
      "17   edu 11.46     like 1.16       drive 1.75\n",
      "17    com 10.4      don 1.12  controller 1.71\n",
      "17    bus 7.24    thanks 1.1         bus 1.56\n",
      "17    don 4.92     does 1.08         isa 1.05\n",
      "17  think 4.64  catbyte 1.05         bit 1.02\n",
      "17   like 4.54  dtmedin 1.05          pc 0.92\n",
      "17    ftp 4.22     ingr 1.03      drives 0.72\n",
      "17    isa 4.06      b30 1.03         mac 0.71\n",
      "Topic 18:\n",
      "           LSA          LDA            NNMF\n",
      "18  just 18.24    know 1.18       file 4.95\n",
      "18   edu 15.47    just 1.16      files 3.37\n",
      "18  file 15.08    like 1.14    program 1.82\n",
      "18   com 12.19     don 1.07     windows 1.8\n",
      "18   does 9.14    does 1.06        ftp 1.66\n",
      "18  right 7.23  thanks 1.04        use 1.09\n",
      "18    ftp 6.36   think 0.94  directory 1.04\n",
      "18  files 5.23  people 0.89     format 1.02\n",
      "18     ax 5.16     use 0.82      image 0.99\n",
      "18   card 5.13    good 0.77        dos 0.96\n",
      "Topic 19:\n",
      "             LSA          LDA          NNMF\n",
      "19  window 12.23    know 1.15      edu 3.04\n",
      "19    space 8.26    just 1.15     mail 2.52\n",
      "19     know 7.66     like 1.1      com 2.03\n",
      "19      new 7.07     don 1.04     list 1.83\n",
      "19      gun 6.75    does 1.02  address 1.56\n",
      "19     card 6.23   thanks 1.0     send 1.45\n",
      "19    jesus 4.76    think 0.9    email 1.39\n",
      "19  problem 4.73  people 0.86   thanks 1.09\n",
      "19    drive 4.66      use 0.8      like 1.0\n",
      "19     list 4.41    good 0.75     know 0.95\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA runtime: 1.0570189 \n",
      "\n",
      "LDA runtime: 393.3073933 \n",
      "\n",
      "NNMF runtime: 12.095451400000002 \n"
     ]
    }
   ],
   "source": [
    "print('LSA runtime: {} \\n'.format(elapsed_lsa))\n",
    "print('LDA runtime: {} \\n'.format(elapsed_lda))\n",
    "print('NNMF runtime: {} '.format(elapsed_nnmf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
