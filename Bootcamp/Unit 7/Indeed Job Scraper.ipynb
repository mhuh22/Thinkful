{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Job Scraper\n",
    "\n",
    "The objective of this program is to scrape/ extract a batch of job postings from Indeed. The program currently allows the user to input multiple queries and locations into the scraper, but otherwise leaves the other settings at their defaults. After specifying the settings, the scraper returns the \n",
    "* title\n",
    "* location\n",
    "* company\n",
    "* salary\n",
    "* synopsis\n",
    "\n",
    "The output will be shown in a preview below, and stored in a local csv file. The local csv file will be used for the main project, analyzing the job postings and be able to gain insight from each posting's synopsis.\n",
    "\n",
    "Link to project: https://github.com/mhuh22/Thinkful/blob/master/Bootcamp/Unit%207/Analysis%20on%20Indeed%20Data%20Scientist%20Postings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(query,metro, n):\n",
    "    \n",
    "    # Target url, and n number of pages to scrape\n",
    "    url = \"https://www.indeed.com/jobs?q={}&l={}&start={}\"\n",
    "    \n",
    "    # Create dataframe that we'll return later\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\", \"Query\", \"Metro\"])\n",
    "    \n",
    "    # Scrape first n pages\n",
    "    for i in range(0,n):\n",
    "        html = requests.get(url.format(query, metro, i))\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        \n",
    "        for each in soup.find_all(class_= \"result\" ):\n",
    "            try: \n",
    "                title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "            except:\n",
    "                title = 'None'\n",
    "            try:\n",
    "                location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "            except:    \n",
    "                try:\n",
    "                    location = each.find('div', {'class':\"location\" }).text.replace('\\n', '')\n",
    "                except:\n",
    "                    location = 'None'\n",
    "            try: \n",
    "                company = each.find(class_='company').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = 'None'\n",
    "            try:\n",
    "                salary = each.find('span', {'class':'no-wrap'}).text\n",
    "            except:\n",
    "                salary = 'None'\n",
    "            # Can also extract url (not sure if it will be used, though)\n",
    "#             try:\n",
    "#                 link = 'https://www.indeed.com/viewjob' + each.find('a', href=True)['href'][7:]\n",
    "#             except:    \n",
    "#                 link = 'None'\n",
    "            synopsis = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "            df = df.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Synopsis':synopsis, 'Query': query, 'Metro': metro}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of positions and locations to search\n",
    "positions = ['data+analyst', 'data+scientist', 'data+engineer']\n",
    "cities = ['united states']\n",
    "n = 100\n",
    "\n",
    "# Create the dataframe\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Scrape all positions in all cities specified, and append to dataframe\n",
    "for position in positions:\n",
    "    for city in cities:\n",
    "        data = pd.concat([data, parse(position, city, n)])\n",
    "\n",
    "# Drop duplicate posts (typically caused by sponsored positions reappearing)\n",
    "data = data.drop_duplicates()\n",
    "        \n",
    "# Preview the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return size of dataframe\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scraped data to a local directory\n",
    "if os.path.exists(\"data/job_board.csv\"):\n",
    "    os.remove(\"data/job_board.csv\")\n",
    "data.to_csv('data/job_board.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
