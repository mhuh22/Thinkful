{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4.4.5. Build your own NLP model\n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "# Overview\n",
    "\n",
    "The data selected for this challenge has been obtained from the gutenberg library in the NLTK package, and for this challenge, the two selected texts are from Gilbert Keith Chesterton, \"Brown\" and \"Thursday\". The point of this challenge is to wrangle the text data using two methods - \n",
    "1. Bag of words -\n",
    "2. Tf-dif - \n",
    "After processing the two works by both methods, the objective will be to see if we can predict which lines of text belong to each novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# Print out all of the other available texts in gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "    \n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "brown_doc = nlp(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(I.)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(The, Absence, of, Mr, Glass, THE, consulting,...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(In, such, a, place, the, sea, had, something,...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(It, must, not, be, supposed, that, Dr, Hood, ...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(These, things, were, there, ,, in, their, pla...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0                                               (I.)  Hamlet\n",
       "1  (The, Absence, of, Mr, Glass, THE, consulting,...  Hamlet\n",
       "2  (In, such, a, place, the, sea, had, something,...  Hamlet\n",
       "3  (It, must, not, be, supposed, that, Dr, Hood, ...  Hamlet\n",
       "4  (These, things, were, there, ,, in, their, pla...  Hamlet"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "brown_sents = [[sent, \"Hamlet\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Macbeth\"] for sent in thursday_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(brown_sents + thursday_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 200 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(200)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(brownwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>but</th>\n",
       "      <th>head</th>\n",
       "      <th>like</th>\n",
       "      <th>house</th>\n",
       "      <th>tower</th>\n",
       "      <th>sir</th>\n",
       "      <th>low</th>\n",
       "      <th>get</th>\n",
       "      <th>detective</th>\n",
       "      <th>wild</th>\n",
       "      <th>...</th>\n",
       "      <th>gogol</th>\n",
       "      <th>believe</th>\n",
       "      <th>face</th>\n",
       "      <th>open</th>\n",
       "      <th>say</th>\n",
       "      <th>garden</th>\n",
       "      <th>policeman</th>\n",
       "      <th>sense</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I.)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, Absence, of, Mr, Glass, THE, consulting,...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, such, a, place, the, sea, had, something,...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(It, must, not, be, supposed, that, Dr, Hood, ...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(These, things, were, there, ,, in, their, pla...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  but head like house tower sir low get detective wild     ...     gogol  \\\n",
       "0   0    0    0     0     0   0   0   0         0    0     ...         0   \n",
       "1   0    0    1     0     0   0   0   0         0    0     ...         0   \n",
       "2   0    0    0     0     0   0   0   0         0    0     ...         0   \n",
       "3   0    0    0     0     0   0   0   0         0    0     ...         0   \n",
       "4   0    0    0     0     0   0   0   0         0    0     ...         0   \n",
       "\n",
       "  believe face open say garden policeman sense  \\\n",
       "0       0    0    0   0      0         0     0   \n",
       "1       0    0    0   0      0         0     0   \n",
       "2       0    0    0   0      0         0     0   \n",
       "3       0    0    0   0      0         0     0   \n",
       "4       0    0    0   0      0         0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                                               (I.)      Hamlet  \n",
       "1  (The, Absence, of, Mr, Glass, THE, consulting,...      Hamlet  \n",
       "2  (In, such, a, place, the, sea, had, something,...      Hamlet  \n",
       "3  (It, must, not, be, supposed, that, Dr, Hood, ...      Hamlet  \n",
       "4  (These, things, were, there, ,, in, their, pla...      Hamlet  \n",
       "\n",
       "[5 rows x 274 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9345362017117742\n",
      "\n",
      "Test set score: 0.7023933402705516\n"
     ]
    }
   ],
   "source": [
    "bow_rfc = ensemble.RandomForestClassifier()\n",
    "train = bow_rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', bow_rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', bow_rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.761970853573907\n",
      "\n",
      "Test set score: 0.719389524800555\n"
     ]
    }
   ],
   "source": [
    "bow_lr = LogisticRegression()\n",
    "train = bow_lr.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', bow_lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', bow_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7092297015961138\n",
      "\n",
      "Test set score: 0.7037807839056538\n"
     ]
    }
   ],
   "source": [
    "bow_clf = ensemble.GradientBoostingClassifier()\n",
    "train = bow_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', bow_clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', bow_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The Wisdom of Father Brown by G . K . Chesterton 1914 ]', 'I .', 'THE consulting - rooms of Dr Orion Hood , the eminent criminologist and specialist in certain moral disorders , lay along the sea - front at Scarborough , in a series of very large and well - lighted french windows , which showed the North Sea like one endless outer wall of blue - green marble .', \"Dr Hood paced the length of his string of apartments , bounded  as the boys ' geographies say  on the east by the North Sea and on the west by the serried ranks of his sociological and criminologist library .\"]\n"
     ]
    }
   ],
   "source": [
    "hamlet = gutenberg.paras('chesterton-brown.txt')\n",
    "\n",
    "#processing\n",
    "hamlet_paras=[]\n",
    "for paragraph in hamlet:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    hamlet_paras.append(' '.join(para))\n",
    "\n",
    "print(hamlet_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 5361\n",
      "Original sentence: Luxury was there: there stood upon a special table eight or ten boxes of the best cigars; but they were built upon a plan so that the strongest were always nearest the wall and the mildest nearest the window.\n",
      "Tf_idf vector: {'saffron': 0.4021302976978583, 'remember': 0.33696011208924276, 'park': 0.3482274417177942, 'poet': 0.3301503198303272, 'second': 0.32888613329883737, 'appearance': 0.3700112365829785, 'marked': 0.41291142085244364, 'place': 0.28092218912100986}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset for vectorization\n",
    "X = sentences[0].map(lambda x: text_cleaner(str(x)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "\n",
    "# Set up parameters for the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 64.85333269748416\n",
      "Component 0:\n",
      "0\n",
      "\"I do,\" said the other \"martyrs.                        0.766513\n",
      "\"What do you do, then?\" he said.                        0.766513\n",
      "he said.                                                0.766513\n",
      "\"I have the keys,\" he said.                             0.766513\n",
      "\" she said.                                             0.766513\n",
      "he said.                                                0.766513\n",
      "\" he said.                                              0.766513\n",
      "\"We were three,\" he said.                               0.766513\n",
      "\"No,\" said Syme. \"                                      0.713410\n",
      "\"No,\" said Syme, \"they are formed along the parade.\"    0.551917\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "\"I do,\" said the other \"martyrs.                                                0.522305\n",
      "\"I have the keys,\" he said.                                                     0.522305\n",
      "he said.                                                                        0.522305\n",
      "\" she said.                                                                     0.522305\n",
      "\"What do you do, then?\" he said.                                                0.522305\n",
      "he said.                                                                        0.522305\n",
      "\" he said.                                                                      0.522305\n",
      "\"We were three,\" he said.                                                       0.522305\n",
      "\"I said nothing at all,\" said the Marquis, \"except something about the band.    0.352327\n",
      "\"I am only on my holiday,\" he said.                                             0.346298\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "\" asked Syme.                                        0.781898\n",
      "\"What was that?\" asked Syme.                         0.781898\n",
      "\"What is his name?\" asked Syme.                      0.781898\n",
      "\"With whom?\" asked Syme. \"                           0.781898\n",
      "\"What do you mean?\" asked Syme.                      0.720074\n",
      "Whom?\" asked Syme quickly. \"                         0.704571\n",
      "\"Join you in what?\" asked Syme.                      0.686391\n",
      "\"Where did you have it?\" asked Syme, wondering. \"    0.616385\n",
      "asked Syme with eager eyes.                          0.597305\n",
      "\"Well?\" asked Syme with a sort of steadiness. \"      0.579823\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "Not yours but I think I know whose.                           0.549870\n",
      "I know what they think I am.                                  0.549870\n",
      "but I think I know him.                                       0.549870\n",
      "I don't think what you think.                                 0.484480\n",
      "\"You don't know how serious this is.\"                         0.483128\n",
      "And we don't know where he is.\"                               0.483128\n",
      "Don't think me rude.                                          0.466114\n",
      "But if you want to know what I don't think, I'll tell you.    0.433060\n",
      "and I know what they mean.                                    0.412834\n",
      "\"I don't think he did it, and you don't either.\"              0.398182\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "All this Father Brown absorbed in detail more at leisure.           0.876918\n",
      "\"There may be,\" said Father Brown agnostically.                     0.734882\n",
      "Father Brown was made of two men.                                   0.711607\n",
      "Father Brown stood ruefully contemplating the decapitated plant.    0.698947\n",
      "\"No,\" answered Father Brown.                                        0.693016\n",
      "\"Nor I,\" said Father Brown faintly.                                 0.685698\n",
      "Have you never heard a ventriloquist?\" asked Father Brown. \"        0.641125\n",
      "\"Who found his body?\" asked Father Brown. \"                         0.617202\n",
      "Father Brown laughed.                                               0.600830\n",
      "Father Brown smiled. \"                                              0.598551\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 5361 to 800.\n",
    "svd= TruncatedSVD(800)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 It was security, eternity I can't convey it...\n",
      "1 \"I think them equally valueless,\" replied Brown. \"\n",
      "2 Rifles were ranked so far away that an enemy could not slink into the town by any detour; therefore it was vain to return to the city by any remote course.\n",
      "3 But, for the last time, where are your goloshes?\n",
      "4 \"Comrade Gregory,\" said the chairman after a painful pause, \"this is really not quite dignified.\n",
      "5 There are others who may remember it because it marked the first appearance in the place of the second poet of Saffron Park.\n",
      "6 Then he said, with a clearly modulated and rather mincing articulation: `Would it discommode you to contribute elsewhere a coin with a somewhat different superscription?' \"\n",
      "7 I am going to pull that meeting's great ugly, mahogany-coloured nose.\"\n",
      "8 On his right was a little wood; far away to his left lay the long curve of the railway line, which he was, so to speak, guarding from the Marquis, whose goal and escape it was.\n",
      "9 He's packing up, I tell you.\"\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9733981031690956\n",
      "\n",
      "Test set score: 0.7356919875130072\n"
     ]
    }
   ],
   "source": [
    "tf_rfc = ensemble.RandomForestClassifier()\n",
    "train = tf_rfc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_rfc.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_rfc.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9218135554013417\n",
      "\n",
      "Test set score: 0.779743322927506\n"
     ]
    }
   ],
   "source": [
    "tf_lr = LogisticRegression()\n",
    "train = tf_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_lr.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_lr.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7404580152671756\n",
      "\n",
      "Test set score: 0.704474505723205\n"
     ]
    }
   ],
   "source": [
    "tf_clf = ensemble.GradientBoostingClassifier()\n",
    "train = tf_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_clf.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_clf.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
