{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning Capstone (name TBA)\n",
    "Author: Matthew Huh\n",
    "    \n",
    "## Overview\n",
    "\n",
    "For the most part, people are free to choose what news outlets they read and follow. In the United States, there is a near-endless list of sites that people can choose from in order to get their daily news and over time, they develop preferences for sites that they are more attached to, and do their best to avoid. Now these affinities are developed through a combination of means ranging from affiliations, vocabulary, prose, and so forth.\n",
    "\n",
    "What I would like to examine in this project is if it is possible to differentiate from several different publications with their respective perks / quirks. \n",
    "\n",
    "## About the Data\n",
    "\n",
    "This dataset was obtained from Kaggle, and contains a collection of 142,570 articles from 15 different publications.\n",
    "\n",
    "The publications within this dataset are\n",
    "1. CNN\n",
    "2. Breitbart\n",
    "3. Vox\n",
    "4. Washington Post\n",
    "5. New York Post\n",
    "6. National Review\n",
    "7. NPR\n",
    "8. Guardian\n",
    "9. Talking Points Memo\n",
    "10. Atlantic\n",
    "11. Reuters\n",
    "12. Fox News\n",
    "13. Business Insider\n",
    "14. Buzzfeed News\n",
    "15. New York Times\n",
    "\n",
    "## Research Question\n",
    "\n",
    "As this is an unsupervised learning project first and foremost, the project will have 3 goals.\n",
    "\n",
    "1. The first goal is to prepare the articles in the dataset for modelling using various Natural Language Processing (NLP) methods to re-represent the data in numbers rather than words\n",
    "2. Cluster the data to determine if we can identify the articles and associate them as different groups.\n",
    "3. Determine if we can predict the structure of the article based on the publisher.\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Clustering packages\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview\n",
    "\n",
    "The first matter of business is to import the articles from a local directory and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of files from directory\n",
    "filelist = os.listdir('articles')\n",
    "\n",
    "# Import the files\n",
    "df_list = [pd.read_csv(file) for file in filelist]\n",
    "\n",
    "#concatenate them together\n",
    "articles = pd.concat(df_list)\n",
    "\n",
    "# Preview the data\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the size of the dataset\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 142,570 articles in the dataset but unfortunately, NLP is quite memory intensive, so we will have to sample the dataset unless you happen to have over 120 GB of memory on your local device. Using a 10% sample still leaves us with 140,000 articles and will be used for the duration of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset for optimal performance\n",
    "articles = articles.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Business Insider', 'New York Post', 'Fox News', 'CNN',\n",
       "       'New York Times', 'Breitbart', 'Talking Points Memo', 'NPR',\n",
       "       'Reuters', 'Guardian', 'Washington Post', 'Vox', 'Atlantic',\n",
       "       'Buzzfeed News', 'National Review'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out unique publisher names\n",
    "articles.publication.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          14247\n",
       "publication       15\n",
       "author          3902\n",
       "date            1040\n",
       "url             8609\n",
       "content        14244\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe unique occurences for each categorical variable\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other ways to trim down the dataset before processing. We aren't particularly interested in examining the dates for this research question, but it may be of interest in another. Let's check to see how many articles each author wrote; it may not be very useful to examine authors that are only responsible for a single article, as different authors from the same publisher may choose compose their works differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables that have no impact on the outcome\n",
    "articles = articles[['title', 'publication', 'author', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Pam Key                                    133\n",
       "Breitbart News                             129\n",
       "Associated Press                           126\n",
       "Jerome Hudson                               85\n",
       "Daniel Nussbaum                             84\n",
       "Charlie Spiering                            78\n",
       "John Hayward                                72\n",
       "Ian Hanchett                                71\n",
       "Camila Domonoske                            70\n",
       "Post Editorial Board                        70\n",
       "AWR Hawkins                                 68\n",
       "Joel B. Pollak                              65\n",
       "Trent Baker                                 57\n",
       "NPR Staff                                   57\n",
       "Warner Todd Huston                          51\n",
       "Alex Swoyer                                 49\n",
       "Reuters                                     47\n",
       "Josh Marshall                               44\n",
       "Jeff Poor                                   44\n",
       "Katherine Rodriguez                         43\n",
       "Breitbart London                            43\n",
       "Esme Cribb                                  41\n",
       "Matthew Yglesias                            40\n",
       "Bill Chappell                               40\n",
       "Ben Kew                                     38\n",
       "German Lopez                                37\n",
       "Jennifer Gould Keil                         35\n",
       "Allan Smith                                 34\n",
       "Merrit Kennedy                              34\n",
       "Claire Atkinson                             34\n",
       "                                          ... \n",
       "Liana B. Baker and Lauren Hirsch             1\n",
       "Liana B. Baker and Greg Roumeliotis          1\n",
       "Liam Stack and Christine Hauser              1\n",
       "Lia Eustachewich and Kate Sheehy             1\n",
       "Leslie Picker and Rachel Abrams              1\n",
       "Leslie Kendall Dye                           1\n",
       "Lisa Barrington                              1\n",
       "Lisa Lambert and Richard Cowan               1\n",
       "Luciana Lopez and Jonathan Allen             1\n",
       "Lisa Rapaport                                1\n",
       "Lucia Mutikani and Susan Heavey              1\n",
       "Lucia Maffei                                 1\n",
       "Lucas Peterson                               1\n",
       "Lt. Gen. (Ret.) Jerry Boykin                 1\n",
       "Loveday Morris                               1\n",
       "Lorenzo Tugnoli                              1\n",
       "Lorenzo Tondo                                1\n",
       "Lola Okolosie                                1\n",
       "Lizzie Parry and Andrea Downey, The Sun      1\n",
       "Lizette Alvarez and Nick Madigan             1\n",
       "Lizbeth Diaz and Frank Jack Daniel           1\n",
       "Liza Graham                                  1\n",
       "Liz Robbins                                  1\n",
       "Liz Lenz                                     1\n",
       "Liz Hampton                                  1\n",
       "Lisandra Paraguassu and Anthony Boadle       1\n",
       "Lisa de Moraes, Deadline                     1\n",
       "Lisa W. Foderaro                             1\n",
       "Lisa Richwine and Arunima Banerjee           1\n",
       " Faith Haleh Robinson                        1\n",
       "Length: 3902, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View most frequently occurring authors\n",
    "articles.groupby(['author']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that partly explains how there are so many authors in this dataset. It seems as though there are over 15,000 authors, and many of them have only published one article, or have co-written multiple articles with other authors. This complicates the problem, so in order to best represent each author's writing style, let's see what happens if we simply remove all authors that only published one article as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "labels": [
          "Business Insider",
          "Fox News",
          "CNN",
          "Breitbart",
          "New York Post",
          "Talking Points Memo",
          "NPR",
          "Washington Post",
          "Reuters",
          "Atlantic",
          "Buzzfeed News",
          "National Review",
          "Vox",
          "New York Times",
          "Guardian"
         ],
         "type": "pie",
         "values": [
          2185,
          1388,
          823,
          719,
          580,
          530,
          530,
          506,
          411,
          403,
          378,
          371,
          263,
          123,
          117
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 600,
        "title": "Articles by Publication",
        "width": 800
       }
      },
      "text/html": [
       "<div id=\"6f92c069-7978-4963-9401-bb249fb15168\" style=\"height: 600px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6f92c069-7978-4963-9401-bb249fb15168\", [{\"type\": \"pie\", \"labels\": [\"Business Insider\", \"Fox News\", \"CNN\", \"Breitbart\", \"New York Post\", \"Talking Points Memo\", \"NPR\", \"Washington Post\", \"Reuters\", \"Atlantic\", \"Buzzfeed News\", \"National Review\", \"Vox\", \"New York Times\", \"Guardian\"], \"values\": [2185, 1388, 823, 719, 580, 530, 530, 506, 411, 403, 378, 371, 263, 123, 117]}], {\"title\": \"Articles by Publication\", \"height\": 600, \"width\": 800, \"autosize\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6f92c069-7978-4963-9401-bb249fb15168\" style=\"height: 600px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6f92c069-7978-4963-9401-bb249fb15168\", [{\"type\": \"pie\", \"labels\": [\"Business Insider\", \"Fox News\", \"CNN\", \"Breitbart\", \"New York Post\", \"Talking Points Memo\", \"NPR\", \"Washington Post\", \"Reuters\", \"Atlantic\", \"Buzzfeed News\", \"National Review\", \"Vox\", \"New York Times\", \"Guardian\"], \"values\": [2185, 1388, 823, 719, 580, 530, 530, 506, 411, 403, 378, 371, 263, 123, 117]}], {\"title\": \"Articles by Publication\", \"height\": 600, \"width\": 800, \"autosize\": false}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotly packages\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import cufflinks as cf\n",
    "import ipywidgets as widgets\n",
    "from scipy import special\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# Pass in values for our pie chart\n",
    "trace = go.Pie(labels=articles['publication'].unique(), values = articles['publication'].value_counts())\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title = 'Articles by Publication',\n",
    "    height = 600,\n",
    "    width = 800,\n",
    "    autosize = False\n",
    ")\n",
    "\n",
    "# Construct the chart\n",
    "fig = go.Figure(data = [trace], layout = layout)\n",
    "py.offline.iplot(fig, filename ='cufflinks/simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop author from the dataframe if they wrote less than 5 articles\n",
    "vc = articles['author'].value_counts()\n",
    "u  = [i not in set(vc[vc<=4].index) for i in articles['author']]\n",
    "articles = articles[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          9318\n",
       "publication      15\n",
       "author          589\n",
       "content        9317\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reprint how many unique authors there are\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9327, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of articles after feature selection\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after removing authors that composed fewer than 5 articles, we are left with 9k articles, or 67% of the data, and roughly 600/3900 of the authors. Now, we can create a better representation of each author since each author has at least 5 articles to evaluate from.\n",
    "\n",
    "## Text Cleaning\n",
    "\n",
    "Now that we've chosen which articles to use, it's time to clean them up and prepare them for feature engineering. What this section covers is the removal of annoying punctuation from the content, and reducing words to their lemmas to reduce the number of words that we are examining. Finally, we'll divide the articles into training and testing sets and separate our predictor, the words in the content, and the target, the publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47986</th>\n",
       "      <td>Donald Trump officially endorses Paul Ryan aft...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Bryan Logan</td>\n",
       "      <td>’ ’ ’ Donald Trump has officially thrown his s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11168</th>\n",
       "      <td>ISIS militants re-enter Syria’s historic Palmyra</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Palmyra Coordination network said the mili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10886</th>\n",
       "      <td>’Don’t tell me’: Georgia man deliberately stay...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“I was invited to an election party to stay up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38584</th>\n",
       "      <td>Possible Ebola exposure in Canadian health lab</td>\n",
       "      <td>CNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(CNN) An employee at a Canadian infectious dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42617</th>\n",
       "      <td>GOP hopefuls split in reactions to same-sex ma...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Tom LoBianco</td>\n",
       "      <td>Washington (CNN) Republicans seeking the White...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title       publication  \\\n",
       "47986  Donald Trump officially endorses Paul Ryan aft...  Business Insider   \n",
       "11168   ISIS militants re-enter Syria’s historic Palmyra          Fox News   \n",
       "10886  ’Don’t tell me’: Georgia man deliberately stay...          Fox News   \n",
       "38584     Possible Ebola exposure in Canadian health lab               CNN   \n",
       "42617  GOP hopefuls split in reactions to same-sex ma...               CNN   \n",
       "\n",
       "             author                                            content  \n",
       "47986   Bryan Logan  ’ ’ ’ Donald Trump has officially thrown his s...  \n",
       "11168           NaN  The Palmyra Coordination network said the mili...  \n",
       "10886           NaN  “I was invited to an election party to stay up...  \n",
       "38584           NaN  (CNN) An employee at a Canadian infectious dis...  \n",
       "42617  Tom LoBianco  Washington (CNN) Republicans seeking the White...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove annoying punctuation from the articles\n",
    "articles['content'] = articles.content.map(lambda x: text_cleaner(str(x)))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Reduce all text to their lemmas\n",
    "for article in articles['content']:\n",
    "    article = lemmatizer.lemmatize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictor and target variables\n",
    "X = articles['content']\n",
    "y = articles['publication']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorization\n",
    "\n",
    "The first types of features that we are going to add are the most useful words in our dataset. Now how are we going to determine which words are deemed the most \"useful\"? With TF-IDF vectorizer, of course.\n",
    "\n",
    "TF tracks the term frequency, or how often each word appears in all articles of text, while idf (or Inverse Document Frequency) is a value that places less weight on variables that occur too often and lose their predictive power. Put together, it's a tool that allows us to assign an importance value to each word in the entire dataset based on frequency in each row and throughout the database.\n",
    "\n",
    "These are the parameters that will be used for TF-IDF\n",
    "1. All words that appear in over half of the articles will be thrown out of the dataframe\n",
    "2. Only words that occur more than 5 times will be tracked\n",
    "3. Only the top 150 features (words) will be kept\n",
    "4. Stop words will be ignored (like, as, the)\n",
    "5. Cases will be ignored\n",
    "6. Shorter and longer articles will be treated equally\n",
    "7. Add 1 to document frequency in case we have to divide by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Parameters for TF-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=5, \n",
    "                             max_features=150, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "# Normalize the dataset    \n",
    "X_norm = normalize(X_train_tfidf)\n",
    "\n",
    "# Convert from tf-idf matrix to dataframe\n",
    "X_normal  = pd.DataFrame(data=X_norm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase count with spacy\n",
    "\n",
    "The second set of variables that we will be creating are counters of how often each publishers makes use of each part of speech, meaning adverbs, verbs, nouns, adjectives, as well as article length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_train_words = []\n",
    "\n",
    "for row in X_train:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])\n",
    "\n",
    "# Create dataframe with count of adverbs, verbs, nouns, and adjectives\n",
    "X_count = pd.DataFrame(data=X_train_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])\n",
    "\n",
    "# Change token count to token percentage\n",
    "for column in X_count.columns[1:5]:\n",
    "    X_count[column] = X_count[column] / X_count['sent_length']\n",
    "\n",
    "# Normalize X_count\n",
    "X_counter = normalize(X_count.drop('BOW',axis=1))\n",
    "X_counter  = pd.DataFrame(data=X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218226</td>\n",
       "      <td>0.233106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227370</td>\n",
       "      <td>0.111283</td>\n",
       "      <td>0.137396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038672</td>\n",
       "      <td>0.101425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.043975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3    4         0    1         2    \\\n",
       "0  0.000126  0.000504  0.000511  0.000161  1.0  0.000000  0.0  0.000000   \n",
       "1  0.000127  0.000463  0.000341  0.000180  1.0  0.000000  0.0  0.000000   \n",
       "2  0.000029  0.000344  0.000383  0.000180  1.0  0.000000  0.0  0.000000   \n",
       "3  0.000041  0.000225  0.000312  0.000092  1.0  0.094248  0.0  0.000000   \n",
       "4  0.000030  0.000172  0.000204  0.000048  1.0  0.000000  0.0  0.042325   \n",
       "\n",
       "        3         4      ...          140       141  142  143       144  \\\n",
       "0  0.372946  0.000000    ...     0.000000  0.000000  0.0  0.0  0.000000   \n",
       "1  0.000000  0.077174    ...     0.000000  0.000000  0.0  0.0  0.065274   \n",
       "2  0.000000  0.000000    ...     0.218226  0.233106  0.0  0.0  0.227370   \n",
       "3  0.038672  0.101425    ...     0.041168  0.043975  0.0  0.0  0.000000   \n",
       "4  0.000000  0.000000    ...     0.076804  0.000000  0.0  0.0  0.000000   \n",
       "\n",
       "        145       146       147       148       149  \n",
       "0  0.000000  0.000000  0.000000  0.170872  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.111283  0.137396  0.000000  0.000000  0.247051  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.034672  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine tf-idf matrix and phrase count matrix\n",
    "features = pd.concat([X_counter,X_normal], ignore_index=False, axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have our list of features. It doesn't look anything like our original dataset, now does it? That's because our sentences have been transformed into numbers to feed into our clustering algorithms and predictive models.\n",
    "\n",
    "# Clustering\n",
    "\n",
    "Now it's finally time for some unsupervised machine learning. Each article has been binarized to 1s and 0s, and it's time to determine if we can determine if each publisher has a different method for publication.\n",
    "\n",
    "### K-means\n",
    "\n",
    "The first clustering method I'll use for modelling the dataset is K-means, that requires the user to input k number of centroids, determining the nearest centroid for each data point, and adjusting the centroids until the best clusters are found, or until a set number of iterations has passed. However, we want to see if we can cluster the articles into 15 clusters representing each of the publishers, so that will be k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>85</td>\n",
       "      <td>16</td>\n",
       "      <td>182</td>\n",
       "      <td>250</td>\n",
       "      <td>29</td>\n",
       "      <td>173</td>\n",
       "      <td>38</td>\n",
       "      <td>57</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "      <td>21</td>\n",
       "      <td>375</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>86</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>138</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>14</td>\n",
       "      <td>119</td>\n",
       "      <td>17</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>17</td>\n",
       "      <td>66</td>\n",
       "      <td>27</td>\n",
       "      <td>68</td>\n",
       "      <td>82</td>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>254</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>121</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                0    1    2    3   4    5   6   7    8   9   10  11  12  \\\n",
       "publication                                                                    \n",
       "Atlantic             13   21   25   47  11   32  16   3   25  13  17  13   8   \n",
       "Breitbart            85   16  182  250  29  173  38  57  107  53  36  89  21   \n",
       "Business Insider     12   12   29   86  50   30   5   8   26   7   5  17   7   \n",
       "Buzzfeed News         3    2    3   19  15   34  14   0   10   0   5  25   2   \n",
       "CNN                  30   19   31   89   5  107  20   2   36   0  16  69  22   \n",
       "Fox News             14    8   40   43   5   46   8   9   11   2   1  38   6   \n",
       "Guardian              8    7    7   41  11   35  11   0   28   0   9  16   2   \n",
       "NPR                  14  119   17   44   8   43  17   2   28  14  18  23   9   \n",
       "National Review      30    7   30   61   2   21  12  13   17  15   8   7   6   \n",
       "New York Post        17   66   27   68  82   73  41   9   19  56  24  74  11   \n",
       "New York Times        3    0    4   13   2    8   4   0    9   4   1   3   4   \n",
       "Reuters               2    0    1    6   7    9   5   0    7  37   0   1   0   \n",
       "Talking Points Memo  18    4   20  114   0   35  18  36   55   3   6  11   3   \n",
       "Vox                  15    6   20   55  11   28   4   2   32  11  10   6   3   \n",
       "Washington Post      19    2   44  121   9   35  19   8   44  12   4  24   8   \n",
       "\n",
       "col_0                 13   14  \n",
       "publication                    \n",
       "Atlantic              49   93  \n",
       "Breitbart            375  122  \n",
       "Business Insider      93   55  \n",
       "Buzzfeed News         34   22  \n",
       "CNN                  138   46  \n",
       "Fox News              49   13  \n",
       "Guardian              54   55  \n",
       "NPR                   68   91  \n",
       "National Review       27   54  \n",
       "New York Post        254  223  \n",
       "New York Times        16   12  \n",
       "Reuters                5    0  \n",
       "Talking Points Memo   60   21  \n",
       "Vox                   20   75  \n",
       "Washington Post       37   19  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(features)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.02422749\n",
      "Silhouette Score: 0.06944205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(features, y_pred, sample_size=60000, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, that doesn't look very good now does it? Based on the clustering above, and our scores, it seems as though it's not very effective. Let's see if there is an issue with what we're measuring by assessing other clustering methods first.\n",
    "\n",
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>114</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>438</td>\n",
       "      <td>245</td>\n",
       "      <td>65</td>\n",
       "      <td>162</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>52</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>164</td>\n",
       "      <td>82</td>\n",
       "      <td>18</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>83</td>\n",
       "      <td>51</td>\n",
       "      <td>26</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>37</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>122</td>\n",
       "      <td>157</td>\n",
       "      <td>38</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>48</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>56</td>\n",
       "      <td>74</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>199</td>\n",
       "      <td>131</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>259</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>92</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "      <td>76</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>101</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                 0    1   2    3    4   5   6   7   8   9   10   11  12  \\\n",
       "publication                                                                    \n",
       "Atlantic              42   49  20   36  114  16  15   9   1  14  11   25  15   \n",
       "Breitbart            438  245  65  162  157  33  10  64  52  38  48  164  82   \n",
       "Business Insider      83   51  26   68   70   2   9  13   8   5   7   27  11   \n",
       "Buzzfeed News         37   44  12   11   22   5   2  21   0  11   0    2   3   \n",
       "CNN                  122  157  38   62   54  12  14  60   2  20   0   30  30   \n",
       "Fox News              48   65  12   38   16   3   7  31   8   6   0   34  14   \n",
       "Guardian              50   50  23   33   63   9   4  13   0  10   0    6   7   \n",
       "NPR                   56   74  32   32  108  16  99  15   3  15  12   14  17   \n",
       "National Review       33   30  14   46   63   7   4   5  12  12  18   29  30   \n",
       "New York Post        199  131  20   45  259  18  51  61   9  38  49   26  16   \n",
       "New York Times        13   14   7   12   15   1   0   2   0   3   3    4   3   \n",
       "Reuters                1   13   6    5    0   0   0   1   0   5  35    1   2   \n",
       "Talking Points Memo   92   38  58   76   34   5   4  10  31  19   4   15  15   \n",
       "Vox                   20   43  35   44   73   8   4   5   2   4  10   19  16   \n",
       "Washington Post       38   52  39  101   26   6   1  19   8  20  13   41  22   \n",
       "\n",
       "col_0                13   14  \n",
       "publication                   \n",
       "Atlantic              7   12  \n",
       "Breitbart            18   57  \n",
       "Business Insider      5   57  \n",
       "Buzzfeed News         2   16  \n",
       "CNN                  20    9  \n",
       "Fox News              5    6  \n",
       "Guardian              1   15  \n",
       "NPR                   7   15  \n",
       "National Review       5    2  \n",
       "New York Post        10  112  \n",
       "New York Times        1    5  \n",
       "Reuters               0   11  \n",
       "Talking Points Memo   3    0  \n",
       "Vox                   3   12  \n",
       "Washington Post       7   12  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SpectralClustering(n_clusters=15)\n",
    "y_pred2 = sc.fit_predict(features)\n",
    "\n",
    "pd.crosstab(y_train, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.02697467\n",
      "Silhouette Score: 0.05824182\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred2)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(features, y_pred2, sample_size=60000, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation\n",
    "\n",
    "Now, for our final attempt at clustering, affinity propagation. It's a method that will group like data points, but most likely result in an excessive number of clusters. Let's see if that can work to our advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                0    1    2    3    4    5    6    7    8    9   ...   \\\n",
       "publication                                                           ...    \n",
       "Atlantic               0    0    0    6    1    1    1    0    3    2 ...    \n",
       "Breitbart              4    0    4    0    4    8    2    6   11   17 ...    \n",
       "Business Insider       1    1    1    1    0    7    1    1    2    6 ...    \n",
       "Buzzfeed News          2    0    0    0    1    0    0    0    1    0 ...    \n",
       "CNN                    2    0    5    3    0    8    1    0    4    5 ...    \n",
       "Fox News               4    0    1    0    0    1    0    1    3    7 ...    \n",
       "Guardian               0    0    1    1    2    0    1    0    1    0 ...    \n",
       "NPR                    2    0    2    4    4    4    2    1    3    3 ...    \n",
       "National Review        0    0    1    0    0    0    2    1    1    5 ...    \n",
       "New York Post         14    2    2    3    2    0    0    0    2    5 ...    \n",
       "New York Times         1    0    0    0    0    0    0    0    1    1 ...    \n",
       "Reuters                0    0    0    0    0    0    0    0    0    1 ...    \n",
       "Talking Points Memo    0    1    1    0    1    0    2    0    1   11 ...    \n",
       "Vox                    2    0    2    1    1    0    3    0    1    4 ...    \n",
       "Washington Post        1    0    0    0    0    0    0    0    5   16 ...    \n",
       "\n",
       "col_0                272  273  274  275  276  277  278  279  280  281  \n",
       "publication                                                            \n",
       "Atlantic               1    1    0    0    2    1    2    1    0    2  \n",
       "Breitbart              1    1    3    1    3   29   13   14    9    8  \n",
       "Business Insider       4    1    1    1    0    4    1    3    0    2  \n",
       "Buzzfeed News          0    6    1    0    1    0    0    0    0    0  \n",
       "CNN                    1    0    0    0    0    7    3    1    0    4  \n",
       "Fox News               0    0    1    0    1    6    5    5    0    3  \n",
       "Guardian               1    0    3    0    1    2    1    0    0    1  \n",
       "NPR                    1    0    1    0    2    6    1    0    1    0  \n",
       "National Review        0    0    0    1    1    1    1    3    5    3  \n",
       "New York Post          2    3    0    5    0    8    2    1    1    5  \n",
       "New York Times         0    0    0    0    0    0    1    0    0    0  \n",
       "Reuters                0    0    0    0    0    0    1    0    0    0  \n",
       "Talking Points Memo    1    0    0    0    2    2    1    4    1    4  \n",
       "Vox                    0    0    1    0    1    2    2    0    0    1  \n",
       "Washington Post        1    2    4    0    1    7    0    1    0    0  \n",
       "\n",
       "[15 rows x 282 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af = AffinityPropagation()\n",
    "y_pred3 = af.fit_predict(features)\n",
    "\n",
    "pd.crosstab(y_train, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.004529963\n",
      "Silhouette Score: -0.03694141\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred3)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(features, y_pred3, sample_size=60000, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the results are worthless, just pitiful. Seems like k-means is the best clustering algorithm- mostly because our other methods were far worse, not because it performed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cluster = pd.DataFrame(features)\n",
    "X_train_cluster['kmeans'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "So now that we attempted clustering with the datset, it's time to run the models.\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier score (without clustering): 0.40743(+/- 0.03)\n",
      "\n",
      "Random forest classifier score (with clustering): 0.41103(+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc_train = cross_val_score(rfc, features, y_train, cv=5, n_jobs=-1)\n",
    "print('Random forest classifier score (without clustering): {:.5f}(+/- {:.2f})\\n'.format(rfc_train.mean(), rfc_train.std()*2))\n",
    "\n",
    "rfc_train_c = cross_val_score(rfc, X_train_cluster, y_train, cv=5, n_jobs=-1)\n",
    "print('Random forest classifier score (with clustering): {:.5f}(+/- {:.2f})'.format(rfc_train_c.mean(), rfc_train_c.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression score (without clustering): 0.43547(+/- 0.03)\n",
      "\n",
      "Logistic regression score (with clustering): 0.43547(+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_train = cross_val_score(lr, features, y_train, cv=5, n_jobs=-1)\n",
    "print('Logistic regression score (without clustering): {:.5f}(+/- {:.2f})\\n'.format(lr_train.mean(), lr_train.std()*2))\n",
    "\n",
    "lr_train_c = cross_val_score(lr, X_train_cluster, y_train, cv=5, n_jobs=-1)\n",
    "print('Logistic regression score (with clustering): {:.5f}(+/- {:.2f})'.format(lr_train_c.mean(), lr_train_c.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting classifier score (without clustering): 0.48250(+/- 0.02)\n",
      "\n",
      "Gradient boosting classifier score (with clustering): 0.48078(+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "gbc_train = cross_val_score(gbc, features, y_train, cv=5, n_jobs=-1)\n",
    "print('Gradient boosting classifier score (without clustering): {:.5f}(+/- {:.2f})\\n'.format(gbc_train.mean(), gbc_train.std()*2))\n",
    "\n",
    "gbc_train_c = cross_val_score(gbc, X_train_cluster, y_train, cv=5, n_jobs=-1)\n",
    "print('Gradient boosting classifier score (with clustering): {:.5f}(+/- {:.2f})'.format(gbc_train_c.mean(), gbc_train_c.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Gradient Boosting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'loss': 'deviance', 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 20, 'n_estimators': 800}\n",
      "Best Score:\n",
      "0.4993566833452466\n"
     ]
    }
   ],
   "source": [
    "# Parameters for gradient boosting classifier\n",
    "param_grid  = {'loss':['deviance'],\n",
    "               'max_features': ['sqrt'],\n",
    "               'n_estimators': [400, 800],\n",
    "               'max_depth': [12, 20],\n",
    "               \"min_samples_leaf\" : [12, 20]}\n",
    "\n",
    "# Run grid search to find ideal parameters\n",
    "gbc_grid = GridSearchCV(gbc, param_grid = param_grid, n_jobs=-1)\n",
    "\n",
    "# Initialize and fit the model.\n",
    "gbc_grid.fit(features, y_train)\n",
    "\n",
    "# Return best parameters and best score\n",
    "print('Best parameters:')\n",
    "print(gbc_grid.best_params_)\n",
    "print('Best Score:')\n",
    "print(gbc_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Tf-idf vectors\n",
    "X_test_norm = normalize(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_words = []\n",
    "\n",
    "for row in X_test:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_test_words.append([row_doc, advs, verb, noun, adj, sent_len])\n",
    "    \n",
    "# Data frame for features\n",
    "X_test_count = pd.DataFrame(data=X_test_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])\n",
    "\n",
    "# Change token count to token percentage\n",
    "for column in X_test_count.columns[1:5]:\n",
    "    X_test_count[column] = X_test_count[column] / X_test_count['sent_length']\n",
    "\n",
    "# Normalize X_count\n",
    "X_test_counter = normalize(X_test_count.drop('BOW',axis=1))\n",
    "X_test_counter  = pd.DataFrame(data=X_test_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079584</td>\n",
       "      <td>0.085011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099928</td>\n",
       "      <td>0.052946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087298</td>\n",
       "      <td>0.046625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.136435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164890</td>\n",
       "      <td>0.118228</td>\n",
       "      <td>0.150292</td>\n",
       "      <td>0.049415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151133</td>\n",
       "      <td>0.072046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.456313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         0         1    \\\n",
       "0  0.000212  0.000750  0.000846  0.000231  0.999999  0.000000  0.000000   \n",
       "1  0.000047  0.000186  0.000221  0.000078  1.000000  0.182195  0.000000   \n",
       "2  0.000028  0.000097  0.000143  0.000049  1.000000  0.099928  0.052946   \n",
       "3  0.000090  0.000368  0.000269  0.000099  1.000000  0.000000  0.000000   \n",
       "4  0.000034  0.000109  0.000109  0.000049  1.000000  0.058145  0.000000   \n",
       "\n",
       "        2         3         4      ...          140       141       142  \\\n",
       "0  0.000000  0.474484  0.000000    ...     0.168367  0.000000  0.000000   \n",
       "1  0.000000  0.037380  0.000000    ...     0.079584  0.085011  0.000000   \n",
       "2  0.000000  0.000000  0.053769    ...     0.087298  0.046625  0.000000   \n",
       "3  0.092247  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.062572    ...     0.000000  0.054259  0.456313   \n",
       "\n",
       "        143       144  145       146       147       148       149  \n",
       "0  0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.0  0.000000  0.000000  0.000000  0.090096  \n",
       "2  0.051634  0.136435  0.0  0.164890  0.118228  0.150292  0.049415  \n",
       "3  0.000000  0.000000  0.0  0.000000  0.151133  0.072046  0.000000  \n",
       "4  0.000000  0.000000  0.0  0.063963  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining features into one data frame\n",
    "X_test_norm_df = pd.DataFrame(data=X_test_norm.toarray())\n",
    "features_test = pd.concat([X_test_counter, X_test_norm_df], ignore_index=False, axis=1)\n",
    "features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "      <td>37</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>79</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>78</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>14</td>\n",
       "      <td>86</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                0   1   2   3   4   5   6   7   8   9   10  11  12  13  \\\n",
       "publication                                                                   \n",
       "Atlantic              3  19  26   2  10  17   9   4   0   3   5   6   2  12   \n",
       "Breitbart             8  36  89  37  58  43  79  18  26   2  30  12   9  78   \n",
       "Business Insider      0  26  21   4   8   7   3   6   1   0   6   2  17  34   \n",
       "Buzzfeed News         2   9   6   9   2   2  10   5   0   0   7   0   7  14   \n",
       "CNN                   7  17  27  17   9  20   6  12   2   4  17   1   1  44   \n",
       "Fox News              0   3   9  12  15   6   3   2   4   3   3   0   3  20   \n",
       "Guardian              3  21  13   8   3   7   4   1   0   0   7   0   4  13   \n",
       "NPR                   9  39  19   5   5  17   4   6   2  47  13   4   6  22   \n",
       "National Review       4  17  27   0  13   5   3   1   3   0  11   4   0   7   \n",
       "New York Post        14  86  21  20   9   8  14  13   2  25   7  18  24  77   \n",
       "New York Times        2  10   4   1   5   4   0   0   0   0   3   6   0   3   \n",
       "Reuters               3   0   7   0   0   1   0   0   0   0   2  17   4   1   \n",
       "Talking Points Memo  10   2  43   3   6   3   5   0  10   1  15   4   0  19   \n",
       "Vox                   2  28  14   3   7   8   4   3   4   1  10   7   2   8   \n",
       "Washington Post       5   6  35   6  12  15   6   5   4   1  13   3   1  11   \n",
       "\n",
       "col_0                14  \n",
       "publication              \n",
       "Atlantic              2  \n",
       "Breitbart            27  \n",
       "Business Insider      3  \n",
       "Buzzfeed News         2  \n",
       "CNN                   9  \n",
       "Fox News              2  \n",
       "Guardian              3  \n",
       "NPR                   6  \n",
       "National Review       6  \n",
       "New York Post         6  \n",
       "New York Times        2  \n",
       "Reuters               2  \n",
       "Talking Points Memo   5  \n",
       "Vox                   4  \n",
       "Washington Post       2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred_test = kmeans.fit_predict(features_test)\n",
    "\n",
    "pd.crosstab(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.02438879\n",
      "Silhouette Score: 0.07111983\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_test, y_pred_test)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(features_test, y_pred_test, sample_size=60000, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test_c = pd.DataFrame(features_test)\n",
    "X2_test_c['kmeans_clust'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.46362(+/- 0.034)\n"
     ]
    }
   ],
   "source": [
    "gbc_grid_scores_test = cross_val_score(gbc_grid, features_test, y_test, cv=5)\n",
    "print('Test set score: {:.5f}(+/- {:.3f})'.format(gbc_grid_scores_test.mean(), gbc_grid_scores_test.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
