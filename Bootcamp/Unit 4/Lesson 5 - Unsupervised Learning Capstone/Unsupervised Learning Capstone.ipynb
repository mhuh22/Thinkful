{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning Capstone (name TBA)\n",
    "Author: Matthew Huh\n",
    "    \n",
    "## Overview\n",
    "\n",
    "For the most part, people are free to choose what news outlets they read and follow. In the United States, there is a near-endless list of sites that people can choose from in order to get their daily news and over time, they develop preferences for sites that they are more attached to, and do their best to avoid. Now these affinities are developed through a combination of means ranging from affiliations, vocabulary, prose, and so forth.\n",
    "\n",
    "What I would like to examine in this project is if it is possible to differentiate from several different publications with their respective perks / quirks. \n",
    "\n",
    "## About the Data\n",
    "\n",
    "This dataset was obtained from Kaggle, and contains a collection of 142,570 articles from 15 different publications.\n",
    "\n",
    "The publications within this dataset are\n",
    "1. CNN\n",
    "2. Breitbart\n",
    "3. Vox\n",
    "4. Washington Post\n",
    "5. New York Post\n",
    "6. National Review\n",
    "7. NPR\n",
    "8. Guardian\n",
    "9. Talking Points Memo\n",
    "10. Atlantic\n",
    "11. Reuters\n",
    "12. Fox News\n",
    "13. Business Insider\n",
    "14. Buzzfeed News\n",
    "15. New York Times\n",
    "\n",
    "## Research Question\n",
    "\n",
    "As this is an unsupervised learning project first and foremost, the project will have 3 goals.\n",
    "\n",
    "1. The first goal is to prepare the articles in the dataset for modelling using various Natural Language Processing (NLP) methods to re-represent the data in numbers rather than words\n",
    "2. Cluster the data to determine if we can identify the articles and associate them as different groups.\n",
    "3. Determine if we can predict the structure of the article based on the publisher.\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Clustering packages\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of files from directory\n",
    "filelist = os.listdir('articles')\n",
    "\n",
    "# Import the files\n",
    "df_list = [pd.read_csv(file) for file in filelist]\n",
    "\n",
    "#concatenate them together\n",
    "articles = pd.concat(df_list)\n",
    "\n",
    "# Preview the data\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the size of the dataset\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample the dataset for optimal performance\n",
    "# articles = articles.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          142132\n",
       "publication        15\n",
       "author          15647\n",
       "date             1646\n",
       "url             85559\n",
       "content        142038\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe unique occurences for each categorical variable\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables that have no impact on the outcome\n",
    "articles = articles[['title', 'publication', 'author', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Breitbart News                                                      1559\n",
       "Pam Key                                                             1282\n",
       "Associated Press                                                    1231\n",
       "Charlie Spiering                                                     928\n",
       "Jerome Hudson                                                        806\n",
       "John Hayward                                                         747\n",
       "Daniel Nussbaum                                                      735\n",
       "AWR Hawkins                                                          720\n",
       "Ian Hanchett                                                         647\n",
       "Joel B. Pollak                                                       624\n",
       "Post Editorial Board                                                 620\n",
       "Alex Swoyer                                                          604\n",
       "Camila Domonoske                                                     593\n",
       "Warner Todd Huston                                                   545\n",
       "NPR Staff                                                            514\n",
       "Jeff Poor                                                            505\n",
       "Merrit Kennedy                                                       484\n",
       "Trent Baker                                                          457\n",
       "Breitbart London                                                     447\n",
       "Katherine Rodriguez                                                  435\n",
       "Reuters                                                              434\n",
       "Charlie Nash                                                         421\n",
       "Bill Chappell                                                        412\n",
       "Ben Kew                                                              373\n",
       "Frances Martel                                                       366\n",
       "David French                                                         363\n",
       "German Lopez                                                         354\n",
       "David A. Graham                                                      351\n",
       "Bob Price                                                            340\n",
       "Esme Cribb                                                           338\n",
       "                                                                    ... \n",
       "Liz Lee                                                                1\n",
       "Liz Hampton and Valerie Volcovici                                      1\n",
       "Liz Hampton and Swetha Gopinath                                        1\n",
       "Liz Hampton and Nia Williams                                           1\n",
       "Liz Hampton and Ethan Lou                                              1\n",
       "Liz Hampton and Ernest Scheyder                                        1\n",
       "Lizette Alvarez and Nick Madigan                                       1\n",
       "Lizette Alvarez and Richard Pérez-Peña                                 1\n",
       "Lorena Mongelli, Daniel Halper and Bruce Golding                       1\n",
       "Lizette Alvarez, Frances Robles and Richard Pérez-Peña                 1\n",
       "Lorena Mongelli and Yaron Steinbuch                                    1\n",
       "Lorena Mongelli and Sophia Rosenbaum                                   1\n",
       "Lorena Mongelli and Max Jaeger                                         1\n",
       "Lorena Mongelli and Gabrielle Fonrouge                                 1\n",
       "Lorena Mongelli and Danika Fears                                       1\n",
       "Lorena Mongelli and Daniel Prendergast                                 1\n",
       "Lorena Mongelli and Chris Perez                                        1\n",
       "Lorena Mongelli                                                        1\n",
       "Lola Adesioye                                                          1\n",
       "Lois Weiss, Stephanie Pagones and Bruce Golding                        1\n",
       "Lois Weiss and Steve Cuozzo                                            1\n",
       "Lois Smith Brady                                                       1\n",
       "Lois Romano                                                            1\n",
       "Logan Hill                                                             1\n",
       "Logan Beirne                                                           1\n",
       "Lizzie Parry and Andrea Downey, The Sun                                1\n",
       "Lizzie Feidelson                                                       1\n",
       "Lizette Alvarez, Richard Fausset and Adam Goldman                      1\n",
       "Lizette Alvarez, Jess Bidgood, Mitch Smith and Sabrina Tavernise       1\n",
       "                                                                       1\n",
       "Length: 15647, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View most frequently occurring authors\n",
    "articles.groupby(['author']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that partly explains how there are so many authors in this dataset. It seems as though there are over 15,000 authors, and many of them have only published one article, or have co-written multiple articles with other authors. This complicates the problem, so in order to best represent each author's writing style, let's see what happens if we simply remove all authors that only published one article as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop author from the dataframe if they wrote less than 5 articles\n",
    "vc = articles['author'].value_counts()\n",
    "u  = [i not in set(vc[vc<=4].index) for i in articles['author']]\n",
    "articles = articles[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          124811\n",
       "publication        15\n",
       "author           3063\n",
       "content        124724\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reprint how many unique authors there are\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125223, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of articles after feature selection\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after removing authors that composed fewer than 5 articles, we are left with 125k articles, or 87.8% of the data, and roughly 3k/15k of the authors. Now, we can create a better representation of each author since each author has at least 5 articles to evaluate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>WASHINGTON — Congressional Republicans have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>SEOUL, South Korea — North Korea’s leader, Kim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Sewell Chan</td>\n",
       "      <td>LONDON — Queen Elizabeth II, who has been batt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Javier C. Hernández</td>\n",
       "      <td>BEIJING — President Tsai of Taiwan sharply cri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     publication  \\\n",
       "0  House Republicans Fret About Winning Their Hea...  New York Times   \n",
       "2  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  New York Times   \n",
       "4  Kim Jong-un Says North Korea Is Preparing to T...  New York Times   \n",
       "5  Sick With a Cold, Queen Elizabeth Misses New Y...  New York Times   \n",
       "6  Taiwan’s President Accuses China of Renewed In...  New York Times   \n",
       "\n",
       "                author                                            content  \n",
       "0           Carl Hulse  WASHINGTON — Congressional Republicans have a ...  \n",
       "2         Margalit Fox  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "4        Choe Sang-Hun  SEOUL, South Korea — North Korea’s leader, Kim...  \n",
       "5          Sewell Chan  LONDON — Queen Elizabeth II, who has been batt...  \n",
       "6  Javier C. Hernández  BEIJING — President Tsai of Taiwan sharply cri...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove annoying punctuation from the articles\n",
    "articles['content'] = articles.content.map(lambda x: text_cleaner(str(x)))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Reduce all text to their lemmas\n",
    "for article in articles['content']:\n",
    "    article = lemmatizer.lemmatize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictor and target variables\n",
    "X = articles['content']\n",
    "y = articles['publication']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=5, # only use words that appear at least twice\n",
    "                             max_features=150, # limit to 300 best features\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "# Normalize the dataset    \n",
    "X_norm = normalize(X_train_tfidf)\n",
    "\n",
    "# Convert from tf-idf matrix to dataframe\n",
    "X_normal  = pd.DataFrame(data=X_norm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase count with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating spaCy\n",
    "# nlp = spacy.load('en')\n",
    "# X_train_words = []\n",
    "\n",
    "# for row in X_train:\n",
    "#     # Processing each row for tokens\n",
    "#     row_doc = nlp(row)\n",
    "#     # Calculating length of each sentence\n",
    "#     sent_len = len(row_doc) \n",
    "#     # Initializing counts of different parts of speech\n",
    "#     advs = 0\n",
    "#     verb = 0\n",
    "#     noun = 0\n",
    "#     adj = 0\n",
    "#     for token in row_doc:\n",
    "#         # Identifying each part of speech and adding to counts\n",
    "#         if token.pos_ == 'ADV':\n",
    "#             advs +=1\n",
    "#         elif token.pos_ == 'VERB':\n",
    "#             verb +=1\n",
    "#         elif token.pos_ == 'NOUN':\n",
    "#             noun +=1\n",
    "#         elif token.pos_ == 'ADJ':\n",
    "#             adj +=1\n",
    "#     # Creating a list of all features for each sentence\n",
    "#     X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])\n",
    "\n",
    "# # Create dataframe with count of adverbs, verbs, nouns, and adjectives\n",
    "# X_count = pd.DataFrame(data=X_train_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])\n",
    "\n",
    "# # Change token count to token percentage\n",
    "# for column in X_count.columns[1:5]:\n",
    "#     X_count[column] = X_count[column] / X_count['sent_length']\n",
    "\n",
    "# # Normalize X_count\n",
    "# X_counter = normalize(X_count.drop('BOW',axis=1))\n",
    "# X_counter  = pd.DataFrame(data=X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine tf-idf matrix and phrase count matrix\n",
    "# features = pd.concat([X_counter,X_normal], ignore_index=False, axis=1)\n",
    "# features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating and fitting the 300 best features\n",
    "# kbest = SelectKBest(f_classif, k=300)\n",
    "# X2_train = kbest.fit_transform(features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>822</td>\n",
       "      <td>195</td>\n",
       "      <td>133</td>\n",
       "      <td>129</td>\n",
       "      <td>813</td>\n",
       "      <td>137</td>\n",
       "      <td>206</td>\n",
       "      <td>327</td>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>1250</td>\n",
       "      <td>147</td>\n",
       "      <td>229</td>\n",
       "      <td>221</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>3111</td>\n",
       "      <td>135</td>\n",
       "      <td>484</td>\n",
       "      <td>294</td>\n",
       "      <td>5013</td>\n",
       "      <td>417</td>\n",
       "      <td>1040</td>\n",
       "      <td>1264</td>\n",
       "      <td>304</td>\n",
       "      <td>259</td>\n",
       "      <td>1470</td>\n",
       "      <td>1055</td>\n",
       "      <td>1943</td>\n",
       "      <td>461</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>844</td>\n",
       "      <td>133</td>\n",
       "      <td>34</td>\n",
       "      <td>696</td>\n",
       "      <td>1224</td>\n",
       "      <td>105</td>\n",
       "      <td>153</td>\n",
       "      <td>213</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>904</td>\n",
       "      <td>137</td>\n",
       "      <td>252</td>\n",
       "      <td>57</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>396</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>1007</td>\n",
       "      <td>116</td>\n",
       "      <td>92</td>\n",
       "      <td>147</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>398</td>\n",
       "      <td>302</td>\n",
       "      <td>108</td>\n",
       "      <td>131</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>1179</td>\n",
       "      <td>222</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>2449</td>\n",
       "      <td>289</td>\n",
       "      <td>404</td>\n",
       "      <td>510</td>\n",
       "      <td>196</td>\n",
       "      <td>9</td>\n",
       "      <td>1195</td>\n",
       "      <td>772</td>\n",
       "      <td>384</td>\n",
       "      <td>214</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>483</td>\n",
       "      <td>58</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>831</td>\n",
       "      <td>113</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>181</td>\n",
       "      <td>333</td>\n",
       "      <td>508</td>\n",
       "      <td>32</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>766</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>1062</td>\n",
       "      <td>269</td>\n",
       "      <td>156</td>\n",
       "      <td>284</td>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "      <td>1541</td>\n",
       "      <td>321</td>\n",
       "      <td>123</td>\n",
       "      <td>220</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>708</td>\n",
       "      <td>2176</td>\n",
       "      <td>144</td>\n",
       "      <td>176</td>\n",
       "      <td>1139</td>\n",
       "      <td>216</td>\n",
       "      <td>204</td>\n",
       "      <td>342</td>\n",
       "      <td>335</td>\n",
       "      <td>14</td>\n",
       "      <td>1555</td>\n",
       "      <td>268</td>\n",
       "      <td>242</td>\n",
       "      <td>271</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>836</td>\n",
       "      <td>60</td>\n",
       "      <td>146</td>\n",
       "      <td>15</td>\n",
       "      <td>529</td>\n",
       "      <td>46</td>\n",
       "      <td>401</td>\n",
       "      <td>516</td>\n",
       "      <td>96</td>\n",
       "      <td>10</td>\n",
       "      <td>662</td>\n",
       "      <td>56</td>\n",
       "      <td>356</td>\n",
       "      <td>141</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>769</td>\n",
       "      <td>766</td>\n",
       "      <td>537</td>\n",
       "      <td>826</td>\n",
       "      <td>2447</td>\n",
       "      <td>643</td>\n",
       "      <td>242</td>\n",
       "      <td>204</td>\n",
       "      <td>108</td>\n",
       "      <td>26</td>\n",
       "      <td>3211</td>\n",
       "      <td>781</td>\n",
       "      <td>306</td>\n",
       "      <td>290</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>88</td>\n",
       "      <td>131</td>\n",
       "      <td>438</td>\n",
       "      <td>141</td>\n",
       "      <td>32</td>\n",
       "      <td>90</td>\n",
       "      <td>81</td>\n",
       "      <td>1837</td>\n",
       "      <td>584</td>\n",
       "      <td>95</td>\n",
       "      <td>68</td>\n",
       "      <td>110</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>498</td>\n",
       "      <td>11</td>\n",
       "      <td>971</td>\n",
       "      <td>680</td>\n",
       "      <td>942</td>\n",
       "      <td>143</td>\n",
       "      <td>168</td>\n",
       "      <td>313</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>137</td>\n",
       "      <td>108</td>\n",
       "      <td>33</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>1240</td>\n",
       "      <td>38</td>\n",
       "      <td>51</td>\n",
       "      <td>23</td>\n",
       "      <td>788</td>\n",
       "      <td>44</td>\n",
       "      <td>215</td>\n",
       "      <td>458</td>\n",
       "      <td>199</td>\n",
       "      <td>11</td>\n",
       "      <td>201</td>\n",
       "      <td>109</td>\n",
       "      <td>225</td>\n",
       "      <td>39</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>671</td>\n",
       "      <td>58</td>\n",
       "      <td>140</td>\n",
       "      <td>80</td>\n",
       "      <td>353</td>\n",
       "      <td>36</td>\n",
       "      <td>149</td>\n",
       "      <td>277</td>\n",
       "      <td>260</td>\n",
       "      <td>7</td>\n",
       "      <td>919</td>\n",
       "      <td>71</td>\n",
       "      <td>207</td>\n",
       "      <td>162</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>1818</td>\n",
       "      <td>92</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>1390</td>\n",
       "      <td>187</td>\n",
       "      <td>413</td>\n",
       "      <td>603</td>\n",
       "      <td>171</td>\n",
       "      <td>134</td>\n",
       "      <td>791</td>\n",
       "      <td>367</td>\n",
       "      <td>461</td>\n",
       "      <td>162</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                  0     1    2    3     4    5     6     7    8     9   \\\n",
       "publication                                                                   \n",
       "Atlantic              822   195  133  129   813  137   206   327   93     8   \n",
       "Breitbart            3111   135  484  294  5013  417  1040  1264  304   259   \n",
       "Business Insider      844   133   34  696  1224  105   153   213   73     6   \n",
       "Buzzfeed News         396    38    1  333  1007  116    92   147  102     7   \n",
       "CNN                  1179   222    4   88  2449  289   404   510  196     9   \n",
       "Fox News              483    58   82   37   831  113   120   188   41    18   \n",
       "Guardian              766   197    0  184  1062  269   156   284   75    21   \n",
       "NPR                   708  2176  144  176  1139  216   204   342  335    14   \n",
       "National Review       836    60  146   15   529   46   401   516   96    10   \n",
       "New York Post         769   766  537  826  2447  643   242   204  108    26   \n",
       "New York Times         53    17   88  131   438  141    32    90   81  1837   \n",
       "Reuters               498    11  971  680   942  143   168   313   55     0   \n",
       "Talking Points Memo  1240    38   51   23   788   44   215   458  199    11   \n",
       "Vox                   671    58  140   80   353   36   149   277  260     7   \n",
       "Washington Post      1818    92  172  173  1390  187   413   603  171   134   \n",
       "\n",
       "col_0                  10    11    12   13   14  \n",
       "publication                                      \n",
       "Atlantic             1250   147   229  221  153  \n",
       "Breitbart            1470  1055  1943  461  396  \n",
       "Business Insider      904   137   252   57   74  \n",
       "Buzzfeed News         398   302   108  131  205  \n",
       "CNN                  1195   772   384  214  284  \n",
       "Fox News              181   333   508   32   89  \n",
       "Guardian             1541   321   123  220  172  \n",
       "NPR                  1555   268   242  271  237  \n",
       "National Review       662    56   356  141  129  \n",
       "New York Post        3211   781   306  290  438  \n",
       "New York Times        584    95    68  110   72  \n",
       "Reuters                79   137   108   33  321  \n",
       "Talking Points Memo   201   109   225   39  154  \n",
       "Vox                   919    71   207  162   81  \n",
       "Washington Post       791   367   461  162  300  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X_normal)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.03274882\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-63e334192731>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Adjusted Rand Score: {:0.7}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Silhouette Score: {:0.7}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mcheck_number_of_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0munique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mn_samples_per_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[1;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_normal, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpectralClustering(n_clusters=15)\n",
    "y_pred3 = sc.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y2_train, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred3)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred3, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = Affinity Propagation\n",
    "y_pred4 = af.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y2_train, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred4)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred4, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rfc = ensemble.RandomForestClassifier()\n",
    "train = tf_rfc.fit(X_normal, y_train)\n",
    "\n",
    "print('Training set score:', tf_rfc.score(X_normal, y_train))\n",
    "print('\\nTest set score:', tf_rfc.score(X_normal, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lr = LogisticRegression()\n",
    "train = tf_lr.fit(X_normal, y_train)\n",
    "\n",
    "print('Training set score:', tf_lr.score(X_normal, y_train))\n",
    "print('\\nTest set score:', tf_lr.score(X_normal, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
