{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning Capstone (name TBA)\n",
    "Author: Matthew Huh\n",
    "    \n",
    "## About the Data\n",
    "\n",
    "Collection of 142,570 articles from 15 different publications...\n",
    "\n",
    "## Research Question\n",
    "\n",
    "...\n",
    "\n",
    "## Overview\n",
    "\n",
    "...\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Clustering packages\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, twitter_samples, gutenberg\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of files from directory\n",
    "filelist = os.listdir('articles')\n",
    "\n",
    "# Import the files\n",
    "df_list = [pd.read_csv(file) for file in filelist]\n",
    "\n",
    "#concatenate them together\n",
    "articles = pd.concat(df_list)\n",
    "\n",
    "# Preview the data\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          14247\n",
       "publication       15\n",
       "author          3917\n",
       "date            1027\n",
       "url             8595\n",
       "content        14238\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables that have no impact on the outcome\n",
    "articles = articles[['title', 'publication', 'author', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Breitbart News                                         147\n",
       "Associated Press                                       127\n",
       "Pam Key                                                127\n",
       "Joel B. Pollak                                          82\n",
       "Camila Domonoske                                        75\n",
       "Jerome Hudson                                           74\n",
       "Daniel Nussbaum                                         73\n",
       "Charlie Spiering                                        71\n",
       "Post Editorial Board                                    67\n",
       "AWR Hawkins                                             66\n",
       "John Hayward                                            66\n",
       "Ian Hanchett                                            63\n",
       "Alex Swoyer                                             62\n",
       "NPR Staff                                               58\n",
       "Warner Todd Huston                                      53\n",
       "Katherine Rodriguez                                     52\n",
       "Reuters                                                 51\n",
       "Merrit Kennedy                                          51\n",
       "Breitbart London                                        46\n",
       "Bill Chappell                                           46\n",
       "Jeff Poor                                               45\n",
       "Charlie Nash                                            44\n",
       "Trent Baker                                             41\n",
       "Ben Kew                                                 39\n",
       "Bob Price                                               36\n",
       "Matthew Yglesias                                        35\n",
       "Frances Martel                                          35\n",
       "Dr. Susan Berry                                         34\n",
       "Thomas D. Williams, Ph.D.                               33\n",
       "Matt Shuham                                             33\n",
       "                                                      ... \n",
       "Laura M. Holson                                          1\n",
       "Laura Italiano, Sophia Rosenbaum and Philip Messing      1\n",
       "Laura Italiano and David K. Li                           1\n",
       "Laura Castañeda                                          1\n",
       "Launa Hall                                               1\n",
       "Lars Gotrich                                             1\n",
       "Larry Kaplow                                             1\n",
       "Leah Askarinam                                           1\n",
       "Leah Bitsky                                              1\n",
       "Leah Sottile                                             1\n",
       "Liam Donovan                                             1\n",
       "Lindsay Dunsmuir and Jason Lange                         1\n",
       "Linda Searing                                            1\n",
       "Linda Qiu                                                1\n",
       "Lin Noueihed and Tim Hepher                              1\n",
       "Lily Koppel                                              1\n",
       "Lidia Kelly and Marcin Goettig                           1\n",
       "Liana B. Baker and Michael Flaherty                      1\n",
       "Liana B. Baker and Jim Finkle                            1\n",
       "Lia Eustachewich and Kate Sheehy                         1\n",
       "Lee Glendinning                                          1\n",
       "Lia Eustachewich and Jamie Schram                        1\n",
       "Lesley Wroughton and Lisa Barrington                     1\n",
       "Leonica Valentine and Jamie Schram                       1\n",
       "Leonica Valentine and Chris Perez                        1\n",
       "Leika Kihara                                             1\n",
       "Leigh Paterson                                           1\n",
       "Lehar Maan                                               1\n",
       "Lee Habeeb                                               1\n",
       " Ravi Ubha                                               1\n",
       "Length: 3917, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.groupby(['author']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that partly explains how there are so many authors in this dataset. It seems as though there are over 15,000 authors, and many of them have only published one article, or have co-written multiple articles with other authors. This complicates the problem, so in order to best represent each author's writing style, let's see what happens if we simply remove all authors that only published one article as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop author from the dataframe if they wrote less than 5 articles\n",
    "vc = articles['author'].value_counts()\n",
    "u  = [i not in set(vc[vc<=4].index) for i in articles['author']]\n",
    "articles = articles[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          9342\n",
       "publication      15\n",
       "author          615\n",
       "content        9334\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reprint how many unique authors there are\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9348, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of articles after feature selection\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after removing authors that composed fewer than 5 articles, we are left with 125k articles, or 87.8% of the data, and roughly 3k/15k of the authors. Now, we can create a better representation of each author since each author has at least 5 articles to evaluate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>Nice attack: French police arrest two more peo...</td>\n",
       "      <td>Guardian</td>\n",
       "      <td>Sam Jones</td>\n",
       "      <td>French police have arrested a man and a woman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15629</th>\n",
       "      <td>New Video Shows LaVoy Finicumâ€™s Last Moments...</td>\n",
       "      <td>Talking Points Memo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cell phone footage taken from the backseat of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7877</th>\n",
       "      <td>Fugitive Paris attacker Salah Abdeslam capture...</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salah Abdeslam, the main fugitive from Islamic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33099</th>\n",
       "      <td>We’re sending too much junk into space</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Mike Wehner, BGR</td>\n",
       "      <td>Space is big and the hardware that humans keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43518</th>\n",
       "      <td>Trump just received the lowest approval rating...</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Brett LoGiurato</td>\n",
       "      <td>’ ’ ’ US President Donald Trump received the l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title          publication  \\\n",
       "3583   Nice attack: French police arrest two more peo...             Guardian   \n",
       "15629  New Video Shows LaVoy Finicumâ€™s Last Moments...  Talking Points Memo   \n",
       "7877   Fugitive Paris attacker Salah Abdeslam capture...             Fox News   \n",
       "33099             We’re sending too much junk into space        New York Post   \n",
       "43518  Trump just received the lowest approval rating...     Business Insider   \n",
       "\n",
       "                 author                                            content  \n",
       "3583          Sam Jones  French police have arrested a man and a woman ...  \n",
       "15629               NaN  Cell phone footage taken from the backseat of ...  \n",
       "7877                NaN  Salah Abdeslam, the main fugitive from Islamic...  \n",
       "33099  Mike Wehner, BGR  Space is big and the hardware that humans keep...  \n",
       "43518   Brett LoGiurato  ’ ’ ’ US President Donald Trump received the l...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['content'] = articles.content.map(lambda x: text_cleaner(str(x)))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for article in articles['content']:\n",
    "    article = lemmatizer.lemmatize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictor and target variables\n",
    "X = articles['content']\n",
    "y = articles['publication']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.99, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7011,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 26856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=5, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7011, 26856)\n",
      "(2337, 26856)\n"
     ]
    }
   ],
   "source": [
    "# Examining shapes \n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "X_norm = normalize(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8c32c3a70899>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Processing each row for tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrow_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Calculating length of each sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msent_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         '''\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         '''\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(seqs_in)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.Ops.flatten\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_train_words = []\n",
    "\n",
    "for row in X_train:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_train_words = []\n",
    "\n",
    "for row in X_train2:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counter = pd.DataFrame(data=X_train_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(America, ’s, central, bank, is, nudging, its,...</td>\n",
       "      <td>42</td>\n",
       "      <td>118</td>\n",
       "      <td>172</td>\n",
       "      <td>59</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(As, time, goes, by, MLB, ’s, home, run, leade...</td>\n",
       "      <td>30</td>\n",
       "      <td>61</td>\n",
       "      <td>96</td>\n",
       "      <td>40</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(A, foreign, government, has, revealed, anothe...</td>\n",
       "      <td>15</td>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>29</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(LAS, VEGAS, —, sensation, point, guard, Chass...</td>\n",
       "      <td>19</td>\n",
       "      <td>72</td>\n",
       "      <td>96</td>\n",
       "      <td>30</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Imagine, if, ,, as, happens, every, thousand,...</td>\n",
       "      <td>94</td>\n",
       "      <td>334</td>\n",
       "      <td>461</td>\n",
       "      <td>181</td>\n",
       "      <td>2328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(President, Donald, Trump, stands, by, his, be...</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>67</td>\n",
       "      <td>27</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(How, close, was, notorious, Mexican, drug, lo...</td>\n",
       "      <td>35</td>\n",
       "      <td>104</td>\n",
       "      <td>124</td>\n",
       "      <td>33</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((, CNN, ), For, the, second, time, in, a, wee...</td>\n",
       "      <td>34</td>\n",
       "      <td>101</td>\n",
       "      <td>98</td>\n",
       "      <td>52</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(”, Everyone, in, business, wants, to, know, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(If, you, ’ve, ever, been, addicted, to, an, a...</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(The, suspect, in, the, shooting, of, an, Idah...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(The, U., S., central, bank, held, interest, r...</td>\n",
       "      <td>19</td>\n",
       "      <td>81</td>\n",
       "      <td>126</td>\n",
       "      <td>53</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Take, a, look, at, 38, photos, of, the, week,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Atlanta, (, CNN, ), Whitney, Houston, ’s, dau...</td>\n",
       "      <td>39</td>\n",
       "      <td>163</td>\n",
       "      <td>166</td>\n",
       "      <td>70</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((, CNN, ), It, has, been, 15, months, since, ...</td>\n",
       "      <td>60</td>\n",
       "      <td>222</td>\n",
       "      <td>262</td>\n",
       "      <td>86</td>\n",
       "      <td>1318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(For, us, to, continue, writing, great, storie...</td>\n",
       "      <td>77</td>\n",
       "      <td>193</td>\n",
       "      <td>156</td>\n",
       "      <td>80</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(After, a, bloody, August, saw, homicides, cli...</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>66</td>\n",
       "      <td>28</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(A, judge, in, Louisiana, has, been, banned, f...</td>\n",
       "      <td>29</td>\n",
       "      <td>112</td>\n",
       "      <td>122</td>\n",
       "      <td>35</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(Trump, lashes, out, at, Clinton, and, Comey, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Thursday, on, MSNBC, ,, Rep., Luis, Gutierrez...</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(Before, he, was, the, 44th, POTUS, and, Miche...</td>\n",
       "      <td>49</td>\n",
       "      <td>163</td>\n",
       "      <td>125</td>\n",
       "      <td>85</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(Once, upon, a, time, ,, cigarettes, were, the...</td>\n",
       "      <td>40</td>\n",
       "      <td>120</td>\n",
       "      <td>171</td>\n",
       "      <td>62</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(WASHINGTON, ,, D., C., —, On, Wednesday, Dire...</td>\n",
       "      <td>21</td>\n",
       "      <td>97</td>\n",
       "      <td>99</td>\n",
       "      <td>35</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(There, are, times, when, ,, as, a, politician...</td>\n",
       "      <td>53</td>\n",
       "      <td>127</td>\n",
       "      <td>137</td>\n",
       "      <td>84</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(CAIRO, —, Military, helicopters, and, police,...</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(LOUISVILLE, ,, Ky., (, AP, ), —, Nyquist, won...</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(When, it, comes, to, reversing, the, obesity,...</td>\n",
       "      <td>36</td>\n",
       "      <td>105</td>\n",
       "      <td>143</td>\n",
       "      <td>38</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(After, what, amounts, to, a, series, of, disa...</td>\n",
       "      <td>222</td>\n",
       "      <td>563</td>\n",
       "      <td>745</td>\n",
       "      <td>390</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(Like, Donald, Trump, ,, Barack, Obama, came, ...</td>\n",
       "      <td>56</td>\n",
       "      <td>103</td>\n",
       "      <td>142</td>\n",
       "      <td>72</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(By, then, ,, Daniel, had, been, in, Afghanist...</td>\n",
       "      <td>190</td>\n",
       "      <td>833</td>\n",
       "      <td>624</td>\n",
       "      <td>239</td>\n",
       "      <td>4322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>(’, ’, ”, Sometimes, I, just, ca, n’t, believe...</td>\n",
       "      <td>36</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "      <td>37</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>(It, seems, DICE, is, done, with, their, inaug...</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "      <td>80</td>\n",
       "      <td>55</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>(On, Tuesday, ,, Michigan, Gov., Rick, Snyder,...</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>90</td>\n",
       "      <td>45</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>(The, former, national, security, adviser, Mic...</td>\n",
       "      <td>35</td>\n",
       "      <td>170</td>\n",
       "      <td>204</td>\n",
       "      <td>81</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>(Legendary, standup, comic, Jackie, Mason, is,...</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>(The, prosecutor, in, the, Stanford, sexual, a...</td>\n",
       "      <td>21</td>\n",
       "      <td>113</td>\n",
       "      <td>150</td>\n",
       "      <td>68</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>((, CNN, ), The, New, York, Public, Library, (...</td>\n",
       "      <td>26</td>\n",
       "      <td>114</td>\n",
       "      <td>136</td>\n",
       "      <td>56</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>(North, Korea, is, “, readying, two, intercont...</td>\n",
       "      <td>20</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>68</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>(’’, ’Anonymous, declaring, ”, total, war, ”, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>(’’, ’As, presidential, candidates, focus, on,...</td>\n",
       "      <td>29</td>\n",
       "      <td>98</td>\n",
       "      <td>153</td>\n",
       "      <td>85</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(WASHINGTON, —, Airplane, seats, have, been, g...</td>\n",
       "      <td>29</td>\n",
       "      <td>117</td>\n",
       "      <td>155</td>\n",
       "      <td>45</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>(Listeners, of, Breitbart, News, Daily, and, B...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>(Dr., Alan, Mendoza, ,, founder, and, Executiv...</td>\n",
       "      <td>41</td>\n",
       "      <td>153</td>\n",
       "      <td>139</td>\n",
       "      <td>58</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>(Jared, Kushner, ,, President, Trump, ’s, land...</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>68</td>\n",
       "      <td>20</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>(For, us, to, continue, writing, great, storie...</td>\n",
       "      <td>189</td>\n",
       "      <td>536</td>\n",
       "      <td>523</td>\n",
       "      <td>292</td>\n",
       "      <td>3007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>(Susan, Rice, is, the, real, version, of, Wood...</td>\n",
       "      <td>134</td>\n",
       "      <td>252</td>\n",
       "      <td>289</td>\n",
       "      <td>168</td>\n",
       "      <td>1729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>(Washington, (, CNN, ), Donald, Trump, ’s, cam...</td>\n",
       "      <td>66</td>\n",
       "      <td>255</td>\n",
       "      <td>256</td>\n",
       "      <td>96</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>(By, day, ,, Chyler, Leigh, plays, Alex, Danve...</td>\n",
       "      <td>20</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>28</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(Tech, entrepreneur, and, of, PayPal, Peter, T...</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>(This, is, the, way, the, Rosetta, ends, :, no...</td>\n",
       "      <td>47</td>\n",
       "      <td>145</td>\n",
       "      <td>171</td>\n",
       "      <td>62</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>(Washington, (, CNN, ), Sen., Amy, Klobuchar, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>(President, Donald, Trump, ’s, Thursday, press...</td>\n",
       "      <td>41</td>\n",
       "      <td>119</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(“, I, was, stripped, of, all, clothing, with,...</td>\n",
       "      <td>64</td>\n",
       "      <td>255</td>\n",
       "      <td>306</td>\n",
       "      <td>178</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(A, report, claims, that, Islamic, State, (, )...</td>\n",
       "      <td>28</td>\n",
       "      <td>107</td>\n",
       "      <td>128</td>\n",
       "      <td>55</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(’’, ”, Cleveland, ’s, police, union, released...</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>(The, US, economy, grew, at, a, pace, of, 1, ....</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>(Sure, ,, I, like, cheese, ,, you, like, chees...</td>\n",
       "      <td>95</td>\n",
       "      <td>246</td>\n",
       "      <td>318</td>\n",
       "      <td>124</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(Rep., Darrell, Issa, (, ), says, he, ’s, ”, n...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(A, newly, released, video, from, Center, for,...</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>116</td>\n",
       "      <td>45</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>(Note, :, NPR, ’s, First, Listen, audio, comes...</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  BOW  ADV  VERB  NOUN  ADJ  \\\n",
       "0   (America, ’s, central, bank, is, nudging, its,...   42   118   172   59   \n",
       "1   (As, time, goes, by, MLB, ’s, home, run, leade...   30    61    96   40   \n",
       "2   (A, foreign, government, has, revealed, anothe...   15    72    83   29   \n",
       "3   (LAS, VEGAS, —, sensation, point, guard, Chass...   19    72    96   30   \n",
       "4   (Imagine, if, ,, as, happens, every, thousand,...   94   334   461  181   \n",
       "5   (President, Donald, Trump, stands, by, his, be...   10    49    67   27   \n",
       "6   (How, close, was, notorious, Mexican, drug, lo...   35   104   124   33   \n",
       "7   ((, CNN, ), For, the, second, time, in, a, wee...   34   101    98   52   \n",
       "8   (”, Everyone, in, business, wants, to, know, w...    0     4     3    0   \n",
       "9   (If, you, ’ve, ever, been, addicted, to, an, a...   14    50    60   21   \n",
       "10  (The, suspect, in, the, shooting, of, an, Idah...    1    19    23    5   \n",
       "11  (The, U., S., central, bank, held, interest, r...   19    81   126   53   \n",
       "12  (Take, a, look, at, 38, photos, of, the, week,...    0     1     3    0   \n",
       "13  (Atlanta, (, CNN, ), Whitney, Houston, ’s, dau...   39   163   166   70   \n",
       "14  ((, CNN, ), It, has, been, 15, months, since, ...   60   222   262   86   \n",
       "15  (For, us, to, continue, writing, great, storie...   77   193   156   80   \n",
       "16  (After, a, bloody, August, saw, homicides, cli...   10    38    66   28   \n",
       "17  (A, judge, in, Louisiana, has, been, banned, f...   29   112   122   35   \n",
       "18  (Trump, lashes, out, at, Clinton, and, Comey, ...    0     1     2    1   \n",
       "19  (Thursday, on, MSNBC, ,, Rep., Luis, Gutierrez...   10    87    55   37   \n",
       "20  (Before, he, was, the, 44th, POTUS, and, Miche...   49   163   125   85   \n",
       "21  (Once, upon, a, time, ,, cigarettes, were, the...   40   120   171   62   \n",
       "22  (WASHINGTON, ,, D., C., —, On, Wednesday, Dire...   21    97    99   35   \n",
       "23  (There, are, times, when, ,, as, a, politician...   53   127   137   84   \n",
       "24  (CAIRO, —, Military, helicopters, and, police,...    9    37    69   27   \n",
       "25  (LOUISVILLE, ,, Ky., (, AP, ), —, Nyquist, won...    4    21    30   12   \n",
       "26  (When, it, comes, to, reversing, the, obesity,...   36   105   143   38   \n",
       "27  (After, what, amounts, to, a, series, of, disa...  222   563   745  390   \n",
       "28  (Like, Donald, Trump, ,, Barack, Obama, came, ...   56   103   142   72   \n",
       "29  (By, then, ,, Daniel, had, been, in, Afghanist...  190   833   624  239   \n",
       "..                                                ...  ...   ...   ...  ...   \n",
       "63  (’, ’, ”, Sometimes, I, just, ca, n’t, believe...   36    77    94   37   \n",
       "64  (It, seems, DICE, is, done, with, their, inaug...   23    89    80   55   \n",
       "65  (On, Tuesday, ,, Michigan, Gov., Rick, Snyder,...   23    54    90   45   \n",
       "66  (The, former, national, security, adviser, Mic...   35   170   204   81   \n",
       "67  (Legendary, standup, comic, Jackie, Mason, is,...   12    58    60   22   \n",
       "68  (The, prosecutor, in, the, Stanford, sexual, a...   21   113   150   68   \n",
       "69  ((, CNN, ), The, New, York, Public, Library, (...   26   114   136   56   \n",
       "70  (North, Korea, is, “, readying, two, intercont...   20   118   127   68   \n",
       "71  (’’, ’Anonymous, declaring, ”, total, war, ”, ...   17    43    46   17   \n",
       "72  (’’, ’As, presidential, candidates, focus, on,...   29    98   153   85   \n",
       "73  (WASHINGTON, —, Airplane, seats, have, been, g...   29   117   155   45   \n",
       "74  (Listeners, of, Breitbart, News, Daily, and, B...   29    57    38   22   \n",
       "75  (Dr., Alan, Mendoza, ,, founder, and, Executiv...   41   153   139   58   \n",
       "76  (Jared, Kushner, ,, President, Trump, ’s, land...    4    51    68   20   \n",
       "77  (For, us, to, continue, writing, great, storie...  189   536   523  292   \n",
       "78  (Susan, Rice, is, the, real, version, of, Wood...  134   252   289  168   \n",
       "79  (Washington, (, CNN, ), Donald, Trump, ’s, cam...   66   255   256   96   \n",
       "80  (By, day, ,, Chyler, Leigh, plays, Alex, Danve...   20    57    58   28   \n",
       "81  (Tech, entrepreneur, and, of, PayPal, Peter, T...   12    30    43    9   \n",
       "82  (This, is, the, way, the, Rosetta, ends, :, no...   47   145   171   62   \n",
       "83  (Washington, (, CNN, ), Sen., Amy, Klobuchar, ...   17    56    40   17   \n",
       "84  (President, Donald, Trump, ’s, Thursday, press...   41   119    94   35   \n",
       "85  (“, I, was, stripped, of, all, clothing, with,...   64   255   306  178   \n",
       "86  (A, report, claims, that, Islamic, State, (, )...   28   107   128   55   \n",
       "87  (’’, ”, Cleveland, ’s, police, union, released...    8    38    55   23   \n",
       "88  (The, US, economy, grew, at, a, pace, of, 1, ....    8    32    53   31   \n",
       "89  (Sure, ,, I, like, cheese, ,, you, like, chees...   95   246   318  124   \n",
       "90  (Rep., Darrell, Issa, (, ), says, he, ’s, ”, n...    1     4     3    0   \n",
       "91  (A, newly, released, video, from, Center, for,...    9    71   116   45   \n",
       "92  (Note, :, NPR, ’s, First, Listen, audio, comes...   18    42    58   25   \n",
       "\n",
       "    sent_length  \n",
       "0           811  \n",
       "1           505  \n",
       "2           538  \n",
       "3           476  \n",
       "4          2328  \n",
       "5           348  \n",
       "6           613  \n",
       "7           607  \n",
       "8            13  \n",
       "9           279  \n",
       "10          131  \n",
       "11          688  \n",
       "12           16  \n",
       "13          979  \n",
       "14         1318  \n",
       "15          998  \n",
       "16          330  \n",
       "17          673  \n",
       "18           13  \n",
       "19          412  \n",
       "20          954  \n",
       "21          775  \n",
       "22          543  \n",
       "23          782  \n",
       "24          296  \n",
       "25          197  \n",
       "26          648  \n",
       "27         3483  \n",
       "28          775  \n",
       "29         4322  \n",
       "..          ...  \n",
       "63          514  \n",
       "64          539  \n",
       "65          380  \n",
       "66         1098  \n",
       "67          345  \n",
       "68          679  \n",
       "69          722  \n",
       "70          721  \n",
       "71          264  \n",
       "72          720  \n",
       "73          658  \n",
       "74          324  \n",
       "75          804  \n",
       "76          381  \n",
       "77         3007  \n",
       "78         1729  \n",
       "79         1454  \n",
       "80          370  \n",
       "81          254  \n",
       "82          865  \n",
       "83          269  \n",
       "84          581  \n",
       "85         1618  \n",
       "86          670  \n",
       "87          262  \n",
       "88          261  \n",
       "89         1458  \n",
       "90           22  \n",
       "91          593  \n",
       "92          297  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normal  = pd.DataFrame(data=X_norm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([X_counter,X_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Instantiating and fitting the 150 best features\n",
    "kbest = SelectKBest(chi2, k=150)\n",
    "X2_train = kbest.fit_transform(X_, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X_train)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_train, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rfc = ensemble.RandomForestClassifier()\n",
    "train = tf_rfc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_rfc.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_rfc.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lr = LogisticRegression()\n",
    "train = tf_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_lr.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_lr.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
