{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning Capstone (name TBA)\n",
    "Author: Matthew Huh\n",
    "    \n",
    "## About the Data\n",
    "\n",
    "Collection of 142,570 articles from 15 different publications...\n",
    "\n",
    "## Research Question\n",
    "\n",
    "...\n",
    "\n",
    "## Overview\n",
    "\n",
    "...\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Clustering packages\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, twitter_samples, gutenberg\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of files from directory\n",
    "filelist = os.listdir('articles')\n",
    "\n",
    "# Import the files\n",
    "df_list = [pd.read_csv(file) for file in filelist]\n",
    "\n",
    "#concatenate them together\n",
    "articles = pd.concat(df_list)\n",
    "\n",
    "# Preview the data\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          14253\n",
       "publication       15\n",
       "author          3896\n",
       "date            1060\n",
       "url             8551\n",
       "content        14245\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables that have no impact on the outcome\n",
    "articles = articles[['title', 'publication', 'author', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Breitbart News                                          161\n",
       "Pam Key                                                 137\n",
       "Associated Press                                        114\n",
       "Charlie Spiering                                         94\n",
       "Jerome Hudson                                            83\n",
       "Daniel Nussbaum                                          71\n",
       "Camila Domonoske                                         71\n",
       "AWR Hawkins                                              71\n",
       "John Hayward                                             65\n",
       "Post Editorial Board                                     64\n",
       "Joel B. Pollak                                           61\n",
       "Ian Hanchett                                             58\n",
       "Alex Swoyer                                              57\n",
       "Merrit Kennedy                                           55\n",
       "Breitbart London                                         54\n",
       "Reuters                                                  52\n",
       "Warner Todd Huston                                       49\n",
       "Jeff Poor                                                48\n",
       "Esme Cribb                                               47\n",
       "NPR Staff                                                47\n",
       "Aaron Klein                                              42\n",
       "Trent Baker                                              42\n",
       "Frances Martel                                           41\n",
       "Charlie Nash                                             39\n",
       "David A. Graham                                          39\n",
       "Katherine Rodriguez                                      38\n",
       "Josh Marshall                                            38\n",
       "Bob Price                                                38\n",
       "Dr. Susan Berry                                          36\n",
       "David French                                             35\n",
       "                                                       ... \n",
       "Laura Ingraham                                            1\n",
       "Lars Gotrich                                              1\n",
       "Larry Mone                                                1\n",
       "Larry Celona, Tom Wilson, Joe Marino and Chris Perez      1\n",
       "Larry Celona, Tom Wilson and Tina Moore                   1\n",
       "Larry Celona, Shawn Cohen and Emily Saul                  1\n",
       "Larry Celona, Natalie Musumeci and Georgett Roberts       1\n",
       "Leanna Garfield                                           1\n",
       "Lee Edwards                                               1\n",
       "Lee Hale                                                  1\n",
       "Lexi Browning                                             1\n",
       "Lindsay Dodgson                                           1\n",
       "Linda Sarsour                                             1\n",
       "Linda Poon                                                1\n",
       "Linda Chavez                                              1\n",
       "Lianna Brinded                                            1\n",
       "Liana B. Baker and Jessica Toonkel                        1\n",
       "Lia Eustachewich and Lorena Mongelli                      1\n",
       "Lia Eustachewich and Jamie Schram                         1\n",
       "Lewis Krauskopf, Rodrigo Campos and Megan Davies          1\n",
       "Leika Kihara and Stanley White                            1\n",
       "Lewis Krauskopf and Lawrence Delevingne                   1\n",
       "Letitia Stein and Bernie Woodall                          1\n",
       "Leslie Picker and Reed Abelson                            1\n",
       "Lesley Wroughton and Enrique Pretel                       1\n",
       "Les Neuhaus                                               1\n",
       "Lenore Skenazy and Sam Schwartz                           1\n",
       "Lenn Robbins                                              1\n",
       "Lela Moore and Sona Patel                                 1\n",
       " Alexandra King                                           1\n",
       "Length: 3896, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.groupby(['author']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that partly explains how there are so many authors in this dataset. It seems as though there are over 15,000 authors, and many of them have only published one article, or have co-written multiple articles with other authors. This complicates the problem, so in order to best represent each author's writing style, let's see what happens if we simply remove all authors that only published one article as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop author from the dataframe if they wrote less than 5 articles\n",
    "vc = articles['author'].value_counts()\n",
    "u  = [i not in set(vc[vc<=4].index) for i in articles['author']]\n",
    "articles = articles[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          9448\n",
       "publication      15\n",
       "author          629\n",
       "content        9443\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reprint how many unique authors there are\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9451, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of articles after feature selection\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after removing authors that composed fewer than 5 articles, we are left with 125k articles, or 87.8% of the data, and roughly 3k/15k of the authors. Now, we can create a better representation of each author since each author has at least 5 articles to evaluate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28643</th>\n",
       "      <td>Donald Trump could severely restrict immigrati...</td>\n",
       "      <td>Vox</td>\n",
       "      <td>Dylan Matthews</td>\n",
       "      <td>In many ways, a Donald Trump administration wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37552</th>\n",
       "      <td>Justice Dept. to North Carolina: Law limiting ...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Matt Zapotosky</td>\n",
       "      <td>The federal government took on North Carolina’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>South Korea’s Top Spies Give New Evidence in P...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>SEOUL, South Korea — Officials from North Kore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>CNN Host Lets Sanders Try Again At Saying How ...</td>\n",
       "      <td>Talking Points Memo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN host Dana Bash told Democratic presidentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18197</th>\n",
       "      <td>Islamic State Magazine: Jesus Was ’A Slave of ...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Frances Martel</td>\n",
       "      <td>In the latest issue of its magazine Dabiq, the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title          publication  \\\n",
       "28643  Donald Trump could severely restrict immigrati...                  Vox   \n",
       "37552  Justice Dept. to North Carolina: Law limiting ...      Washington Post   \n",
       "1413   South Korea’s Top Spies Give New Evidence in P...       New York Times   \n",
       "16138  CNN Host Lets Sanders Try Again At Saying How ...  Talking Points Memo   \n",
       "18197  Islamic State Magazine: Jesus Was ’A Slave of ...            Breitbart   \n",
       "\n",
       "               author                                            content  \n",
       "28643  Dylan Matthews  In many ways, a Donald Trump administration wo...  \n",
       "37552  Matt Zapotosky  The federal government took on North Carolina’...  \n",
       "1413    Choe Sang-Hun  SEOUL, South Korea — Officials from North Kore...  \n",
       "16138             NaN  CNN host Dana Bash told Democratic presidentia...  \n",
       "18197  Frances Martel  In the latest issue of its magazine Dabiq, the...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['content'] = articles.content.map(lambda x: text_cleaner(str(x)))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Reduce all text to their lemmas\n",
    "for article in articles['content']:\n",
    "    article = lemmatizer.lemmatize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictor and target variables\n",
    "X = articles['content']\n",
    "y = articles['publication']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.99, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=5, # only use words that appear at least twice\n",
    "                             max_features=3000, # limit to 3000 best features\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "X_norm = normalize(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating spaCy\n",
    "nlp = spacy.load('en')\n",
    "X_train_words = []\n",
    "\n",
    "for row in X_train:\n",
    "    # Processing each row for tokens\n",
    "    row_doc = nlp(row)\n",
    "    # Calculating length of each sentence\n",
    "    sent_len = len(row_doc) \n",
    "    # Initializing counts of different parts of speech\n",
    "    advs = 0\n",
    "    verb = 0\n",
    "    noun = 0\n",
    "    adj = 0\n",
    "    for token in row_doc:\n",
    "        # Identifying each part of speech and adding to counts\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs +=1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb +=1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun +=1\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            adj +=1\n",
    "    # Creating a list of all features for each sentence\n",
    "    X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counter = pd.DataFrame(data=X_train_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(’’, ’Things, are, rough, over, at, Intel, ., ...</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>21</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Monica, Crowley, ,, the, former, Fox, News, c...</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(A, migrant, has, threatened, to, throw, a, yo...</td>\n",
       "      <td>11</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Two, days, after, Donald, Trump, dismissed, r...</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(According, to, the, Archbishop, of, Aleppo, ,...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BOW  ADV  VERB  NOUN  ADJ  \\\n",
       "0  (’’, ’Things, are, rough, over, at, Intel, ., ...   17    47    56   21   \n",
       "1  (Monica, Crowley, ,, the, former, Fox, News, c...   10    41    51   24   \n",
       "2  (A, migrant, has, threatened, to, throw, a, yo...   11    79    90   25   \n",
       "3  (Two, days, after, Donald, Trump, dismissed, r...   32   128   114   57   \n",
       "4  (According, to, the, Archbishop, of, Aleppo, ,...    6    11     6    5   \n",
       "\n",
       "   sent_length  \n",
       "0          342  \n",
       "1          277  \n",
       "2          409  \n",
       "3          718  \n",
       "4           54  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normal  = pd.DataFrame(data=X_norm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOW</th>\n",
       "      <th>ADV</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>2990</th>\n",
       "      <th>2991</th>\n",
       "      <th>2992</th>\n",
       "      <th>2993</th>\n",
       "      <th>2994</th>\n",
       "      <th>2995</th>\n",
       "      <th>2996</th>\n",
       "      <th>2997</th>\n",
       "      <th>2998</th>\n",
       "      <th>2999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(’’, ’Things, are, rough, over, at, Intel, ., ...</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>21</td>\n",
       "      <td>342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Monica, Crowley, ,, the, former, Fox, News, c...</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(A, migrant, has, threatened, to, throw, a, yo...</td>\n",
       "      <td>11</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Two, days, after, Donald, Trump, dismissed, r...</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>114</td>\n",
       "      <td>57</td>\n",
       "      <td>718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(According, to, the, Archbishop, of, Aleppo, ,...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 BOW  ADV  VERB  NOUN  ADJ  \\\n",
       "0  (’’, ’Things, are, rough, over, at, Intel, ., ...   17    47    56   21   \n",
       "1  (Monica, Crowley, ,, the, former, Fox, News, c...   10    41    51   24   \n",
       "2  (A, migrant, has, threatened, to, throw, a, yo...   11    79    90   25   \n",
       "3  (Two, days, after, Donald, Trump, dismissed, r...   32   128   114   57   \n",
       "4  (According, to, the, Archbishop, of, Aleppo, ,...    6    11     6    5   \n",
       "\n",
       "   sent_length    0         1         2    3  ...   2990      2991      2992  \\\n",
       "0          342  0.0  0.000000  0.071358  0.0  ...    0.0  0.000000  0.000000   \n",
       "1          277  0.0  0.000000  0.000000  0.0  ...    0.0  0.061457  0.000000   \n",
       "2          409  0.0  0.079689  0.000000  0.0  ...    0.0  0.000000  0.047811   \n",
       "3          718  0.0  0.000000  0.000000  0.0  ...    0.0  0.033556  0.000000   \n",
       "4           54  0.0  0.000000  0.000000  0.0  ...    0.0  0.000000  0.000000   \n",
       "\n",
       "   2993  2994  2995  2996  2997  2998  2999  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 3006 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.concat([X_counter,X_normal], ignore_index=False, axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop('BOW', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Instantiating and fitting the 150 best features\n",
    "kbest = SelectKBest(f_classif, k=300)\n",
    "X2_train = kbest.fit_transform(features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7088x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 989371 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>132</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>72</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>162</td>\n",
       "      <td>44</td>\n",
       "      <td>217</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>25</td>\n",
       "      <td>217</td>\n",
       "      <td>28</td>\n",
       "      <td>219</td>\n",
       "      <td>443</td>\n",
       "      <td>39</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>74</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>117</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>83</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>61</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "      <td>85</td>\n",
       "      <td>229</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>197</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>124</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>85</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>52</td>\n",
       "      <td>132</td>\n",
       "      <td>39</td>\n",
       "      <td>362</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                0   1    2    3   4    5   6   7   8    9    10   11  \\\n",
       "publication                                                                 \n",
       "Atlantic             14   9  132   16   8    1   0  11  12   75   12   36   \n",
       "Breitbart            32  41   74  162  44  217   5  92  25  217   28  219   \n",
       "Business Insider     74  19   21   12  11    1   2   9  15   46    7   23   \n",
       "Buzzfeed News        18   3   10    5   5    0   4  24   1   21    3   14   \n",
       "CNN                   4  26   61   38  25    1  17  63   0   75   37   85   \n",
       "Fox News              5   5    5   53   4    0   1  36   7   28    6   32   \n",
       "Guardian              8   9   38    8   8    0   1  15   0   34   25   28   \n",
       "NPR                  19  15  197   22  16    1   7  17   9   39   12   55   \n",
       "National Review       2   7   69   27   8    2   2   4  13   54    2   24   \n",
       "New York Post        85  16  128   16   6    1   6  53  37   52  132   39   \n",
       "New York Times        7   0    4    3   2    1   4   6   2    6    5   14   \n",
       "Reuters              24   6    0    4   5    0   7   5  54   10    1   14   \n",
       "Talking Points Memo   5  46    8   21  39    1   1   9   3   83    1   13   \n",
       "Vox                  20  10   71   20  14    0   1   7  12   57    9   21   \n",
       "Washington Post       8  26   26   41  22    3  10  15  12   99    1   24   \n",
       "\n",
       "col_0                 12  13  14  \n",
       "publication                       \n",
       "Atlantic              72  21   5  \n",
       "Breitbart            443  39  58  \n",
       "Business Insider     117   7   8  \n",
       "Buzzfeed News         83  27   1  \n",
       "CNN                  229  15   7  \n",
       "Fox News             104   8  21  \n",
       "Guardian              66   9   0  \n",
       "NPR                  124  27  11  \n",
       "National Review       44  15  14  \n",
       "New York Post        362  30   5  \n",
       "New York Times        18   2   0  \n",
       "Reuters                8   9   1  \n",
       "Talking Points Memo  108  21  27  \n",
       "Vox                   57   5   8  \n",
       "Washington Post       60  22  15  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X_norm)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.02979696\n",
      "Silhouette Score: 0.01446115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_norm, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9928047404063205\n",
      "\n",
      "Test set score: 0.4879390605162928\n"
     ]
    }
   ],
   "source": [
    "tf_rfc = ensemble.RandomForestClassifier()\n",
    "train = tf_rfc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_rfc.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_rfc.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7128950338600452\n",
      "\n",
      "Test set score: 0.5450698264917477\n"
     ]
    }
   ],
   "source": [
    "tf_lr = LogisticRegression()\n",
    "train = tf_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', tf_lr.score(X_train_tfidf, y_train))\n",
    "print('\\nTest set score:', tf_lr.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
