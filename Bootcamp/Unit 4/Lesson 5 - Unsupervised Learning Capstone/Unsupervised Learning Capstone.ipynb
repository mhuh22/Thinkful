{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning Capstone (name TBA)\n",
    "Author: Matthew Huh\n",
    "    \n",
    "## Overview\n",
    "\n",
    "For the most part, people are free to choose what news outlets they read and follow. In the United States, there is a near-endless list of sites that people can choose from in order to get their daily news and over time, they develop preferences for sites that they are more attached to, and do their best to avoid. Now these affinities are developed through a combination of means ranging from affiliations, vocabulary, prose, and so forth.\n",
    "\n",
    "What I would like to examine in this project is if it is possible to differentiate from several different publications with their respective perks / quirks. \n",
    "\n",
    "## About the Data\n",
    "\n",
    "This dataset was obtained from Kaggle, and contains a collection of 142,570 articles from 15 different publications.\n",
    "\n",
    "The publications within this dataset are\n",
    "1. CNN\n",
    "2. Breitbart\n",
    "3. Vox\n",
    "4. Washington Post\n",
    "5. New York Post\n",
    "6. National Review\n",
    "7. NPR\n",
    "8. Guardian\n",
    "9. Talking Points Memo\n",
    "10. Atlantic\n",
    "11. Reuters\n",
    "12. Fox News\n",
    "13. Business Insider\n",
    "14. Buzzfeed News\n",
    "15. New York Times\n",
    "\n",
    "## Research Question\n",
    "\n",
    "As this is an unsupervised learning project first and foremost, the project will have 3 goals.\n",
    "\n",
    "1. The first goal is to prepare the articles in the dataset for modelling using various Natural Language Processing (NLP) methods to re-represent the data in numbers rather than words\n",
    "2. Cluster the data to determine if we can identify the articles and associate them as different groups.\n",
    "3. Determine if we can predict the structure of the article based on the publisher.\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Clustering packages\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Natural Language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                              title  \\\n",
       "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "      publication                         author        date    year  month  \\\n",
       "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
       "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
       "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
       "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
       "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
       "\n",
       "   url                                            content  \n",
       "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1  NaN  After the bullet shells get counted, the blood...  \n",
       "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of files from directory\n",
    "filelist = os.listdir('articles')\n",
    "\n",
    "# Import the files\n",
    "df_list = [pd.read_csv(file) for file in filelist]\n",
    "\n",
    "#concatenate them together\n",
    "articles = pd.concat(df_list)\n",
    "\n",
    "# Preview the data\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142570, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the size of the dataset\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample the dataset for optimal performance\n",
    "# articles = articles.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          142132\n",
       "publication        15\n",
       "author          15647\n",
       "date             1646\n",
       "url             85559\n",
       "content        142038\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe unique occurences for each categorical variable\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables that have no impact on the outcome\n",
    "articles = articles[['title', 'publication', 'author', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Breitbart News                                                      1559\n",
       "Pam Key                                                             1282\n",
       "Associated Press                                                    1231\n",
       "Charlie Spiering                                                     928\n",
       "Jerome Hudson                                                        806\n",
       "John Hayward                                                         747\n",
       "Daniel Nussbaum                                                      735\n",
       "AWR Hawkins                                                          720\n",
       "Ian Hanchett                                                         647\n",
       "Joel B. Pollak                                                       624\n",
       "Post Editorial Board                                                 620\n",
       "Alex Swoyer                                                          604\n",
       "Camila Domonoske                                                     593\n",
       "Warner Todd Huston                                                   545\n",
       "NPR Staff                                                            514\n",
       "Jeff Poor                                                            505\n",
       "Merrit Kennedy                                                       484\n",
       "Trent Baker                                                          457\n",
       "Breitbart London                                                     447\n",
       "Katherine Rodriguez                                                  435\n",
       "Reuters                                                              434\n",
       "Charlie Nash                                                         421\n",
       "Bill Chappell                                                        412\n",
       "Ben Kew                                                              373\n",
       "Frances Martel                                                       366\n",
       "David French                                                         363\n",
       "German Lopez                                                         354\n",
       "David A. Graham                                                      351\n",
       "Bob Price                                                            340\n",
       "Esme Cribb                                                           338\n",
       "                                                                    ... \n",
       "Liz Lee                                                                1\n",
       "Liz Hampton and Valerie Volcovici                                      1\n",
       "Liz Hampton and Swetha Gopinath                                        1\n",
       "Liz Hampton and Nia Williams                                           1\n",
       "Liz Hampton and Ethan Lou                                              1\n",
       "Liz Hampton and Ernest Scheyder                                        1\n",
       "Lizette Alvarez and Nick Madigan                                       1\n",
       "Lizette Alvarez and Richard Pérez-Peña                                 1\n",
       "Lorena Mongelli, Daniel Halper and Bruce Golding                       1\n",
       "Lizette Alvarez, Frances Robles and Richard Pérez-Peña                 1\n",
       "Lorena Mongelli and Yaron Steinbuch                                    1\n",
       "Lorena Mongelli and Sophia Rosenbaum                                   1\n",
       "Lorena Mongelli and Max Jaeger                                         1\n",
       "Lorena Mongelli and Gabrielle Fonrouge                                 1\n",
       "Lorena Mongelli and Danika Fears                                       1\n",
       "Lorena Mongelli and Daniel Prendergast                                 1\n",
       "Lorena Mongelli and Chris Perez                                        1\n",
       "Lorena Mongelli                                                        1\n",
       "Lola Adesioye                                                          1\n",
       "Lois Weiss, Stephanie Pagones and Bruce Golding                        1\n",
       "Lois Weiss and Steve Cuozzo                                            1\n",
       "Lois Smith Brady                                                       1\n",
       "Lois Romano                                                            1\n",
       "Logan Hill                                                             1\n",
       "Logan Beirne                                                           1\n",
       "Lizzie Parry and Andrea Downey, The Sun                                1\n",
       "Lizzie Feidelson                                                       1\n",
       "Lizette Alvarez, Richard Fausset and Adam Goldman                      1\n",
       "Lizette Alvarez, Jess Bidgood, Mitch Smith and Sabrina Tavernise       1\n",
       "                                                                       1\n",
       "Length: 15647, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View most frequently occurring authors\n",
    "articles.groupby(['author']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that partly explains how there are so many authors in this dataset. It seems as though there are over 15,000 authors, and many of them have only published one article, or have co-written multiple articles with other authors. This complicates the problem, so in order to best represent each author's writing style, let's see what happens if we simply remove all authors that only published one article as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop author from the dataframe if they wrote less than 5 articles\n",
    "vc = articles['author'].value_counts()\n",
    "u  = [i not in set(vc[vc<=4].index) for i in articles['author']]\n",
    "articles = articles[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          124811\n",
       "publication        15\n",
       "author           3063\n",
       "content        124724\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reprint how many unique authors there are\n",
    "articles.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125223, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of articles after feature selection\n",
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after removing authors that composed fewer than 5 articles, we are left with 125k articles, or 87.8% of the data, and roughly 3k/15k of the authors. Now, we can create a better representation of each author since each author has at least 5 articles to evaluate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>WASHINGTON — Congressional Republicans have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>SEOUL, South Korea — North Korea’s leader, Kim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Sewell Chan</td>\n",
       "      <td>LONDON — Queen Elizabeth II, who has been batt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Javier C. Hernández</td>\n",
       "      <td>BEIJING — President Tsai of Taiwan sharply cri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     publication  \\\n",
       "0  House Republicans Fret About Winning Their Hea...  New York Times   \n",
       "2  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  New York Times   \n",
       "4  Kim Jong-un Says North Korea Is Preparing to T...  New York Times   \n",
       "5  Sick With a Cold, Queen Elizabeth Misses New Y...  New York Times   \n",
       "6  Taiwan’s President Accuses China of Renewed In...  New York Times   \n",
       "\n",
       "                author                                            content  \n",
       "0           Carl Hulse  WASHINGTON — Congressional Republicans have a ...  \n",
       "2         Margalit Fox  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "4        Choe Sang-Hun  SEOUL, South Korea — North Korea’s leader, Kim...  \n",
       "5          Sewell Chan  LONDON — Queen Elizabeth II, who has been batt...  \n",
       "6  Javier C. Hernández  BEIJING — President Tsai of Taiwan sharply cri...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove annoying punctuation from the articles\n",
    "articles['content'] = articles.content.map(lambda x: text_cleaner(str(x)))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Reduce all text to their lemmas\n",
    "for article in articles['content']:\n",
    "    article = lemmatizer.lemmatize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify predictor and target variables\n",
    "X = articles['content']\n",
    "y = articles['publication']\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=5, # only use words that appear at least twice\n",
    "                             max_features=100, # limit to 150 best features\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "X_tfidf=vectorizer.fit_transform(X)\n",
    "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#Removes all zeros from the matrix\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "# Normalize the dataset    \n",
    "X_norm = normalize(X_train_tfidf)\n",
    "\n",
    "# Convert from tf-idf matrix to dataframe\n",
    "X_normal  = pd.DataFrame(data=X_norm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase count with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating spaCy\n",
    "# nlp = spacy.load('en')\n",
    "# X_train_words = []\n",
    "\n",
    "# for row in X_train:\n",
    "#     # Processing each row for tokens\n",
    "#     row_doc = nlp(row)\n",
    "#     # Calculating length of each sentence\n",
    "#     sent_len = len(row_doc) \n",
    "#     # Initializing counts of different parts of speech\n",
    "#     advs = 0\n",
    "#     verb = 0\n",
    "#     noun = 0\n",
    "#     adj = 0\n",
    "#     for token in row_doc:\n",
    "#         # Identifying each part of speech and adding to counts\n",
    "#         if token.pos_ == 'ADV':\n",
    "#             advs +=1\n",
    "#         elif token.pos_ == 'VERB':\n",
    "#             verb +=1\n",
    "#         elif token.pos_ == 'NOUN':\n",
    "#             noun +=1\n",
    "#         elif token.pos_ == 'ADJ':\n",
    "#             adj +=1\n",
    "#     # Creating a list of all features for each sentence\n",
    "#     X_train_words.append([row_doc, advs, verb, noun, adj, sent_len])\n",
    "\n",
    "# # Create dataframe with count of adverbs, verbs, nouns, and adjectives\n",
    "# X_count = pd.DataFrame(data=X_train_words, columns=['BOW', 'ADV', 'VERB', 'NOUN', 'ADJ', 'sent_length'])\n",
    "\n",
    "# # Change token count to token percentage\n",
    "# for column in X_count.columns[1:5]:\n",
    "#     X_count[column] = X_count[column] / X_count['sent_length']\n",
    "\n",
    "# # Normalize X_count\n",
    "# X_counter = normalize(X_count.drop('BOW',axis=1))\n",
    "# X_counter  = pd.DataFrame(data=X_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine tf-idf matrix and phrase count matrix\n",
    "# features = pd.concat([X_counter,X_normal], ignore_index=False, axis=1)\n",
    "# features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiating and fitting the 300 best features\n",
    "# kbest = SelectKBest(f_classif, k=300)\n",
    "# X2_train = kbest.fit_transform(features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atlantic</th>\n",
       "      <td>203</td>\n",
       "      <td>863</td>\n",
       "      <td>235</td>\n",
       "      <td>159</td>\n",
       "      <td>1183</td>\n",
       "      <td>204</td>\n",
       "      <td>9</td>\n",
       "      <td>163</td>\n",
       "      <td>131</td>\n",
       "      <td>333</td>\n",
       "      <td>102</td>\n",
       "      <td>222</td>\n",
       "      <td>153</td>\n",
       "      <td>161</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breitbart</th>\n",
       "      <td>1043</td>\n",
       "      <td>3193</td>\n",
       "      <td>1968</td>\n",
       "      <td>429</td>\n",
       "      <td>1432</td>\n",
       "      <td>146</td>\n",
       "      <td>276</td>\n",
       "      <td>431</td>\n",
       "      <td>296</td>\n",
       "      <td>1375</td>\n",
       "      <td>307</td>\n",
       "      <td>455</td>\n",
       "      <td>1093</td>\n",
       "      <td>517</td>\n",
       "      <td>4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Insider</th>\n",
       "      <td>153</td>\n",
       "      <td>874</td>\n",
       "      <td>258</td>\n",
       "      <td>114</td>\n",
       "      <td>883</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>696</td>\n",
       "      <td>244</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>35</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buzzfeed News</th>\n",
       "      <td>93</td>\n",
       "      <td>402</td>\n",
       "      <td>105</td>\n",
       "      <td>118</td>\n",
       "      <td>391</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>216</td>\n",
       "      <td>325</td>\n",
       "      <td>176</td>\n",
       "      <td>106</td>\n",
       "      <td>136</td>\n",
       "      <td>318</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>420</td>\n",
       "      <td>1260</td>\n",
       "      <td>399</td>\n",
       "      <td>326</td>\n",
       "      <td>1202</td>\n",
       "      <td>264</td>\n",
       "      <td>18</td>\n",
       "      <td>315</td>\n",
       "      <td>111</td>\n",
       "      <td>581</td>\n",
       "      <td>216</td>\n",
       "      <td>221</td>\n",
       "      <td>824</td>\n",
       "      <td>6</td>\n",
       "      <td>2036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fox News</th>\n",
       "      <td>124</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>117</td>\n",
       "      <td>201</td>\n",
       "      <td>68</td>\n",
       "      <td>20</td>\n",
       "      <td>101</td>\n",
       "      <td>38</td>\n",
       "      <td>195</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>351</td>\n",
       "      <td>87</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guardian</th>\n",
       "      <td>154</td>\n",
       "      <td>800</td>\n",
       "      <td>122</td>\n",
       "      <td>300</td>\n",
       "      <td>1405</td>\n",
       "      <td>214</td>\n",
       "      <td>28</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>311</td>\n",
       "      <td>88</td>\n",
       "      <td>230</td>\n",
       "      <td>337</td>\n",
       "      <td>0</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPR</th>\n",
       "      <td>197</td>\n",
       "      <td>744</td>\n",
       "      <td>252</td>\n",
       "      <td>233</td>\n",
       "      <td>1494</td>\n",
       "      <td>2267</td>\n",
       "      <td>19</td>\n",
       "      <td>255</td>\n",
       "      <td>180</td>\n",
       "      <td>347</td>\n",
       "      <td>330</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>160</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>National Review</th>\n",
       "      <td>400</td>\n",
       "      <td>879</td>\n",
       "      <td>377</td>\n",
       "      <td>46</td>\n",
       "      <td>604</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>131</td>\n",
       "      <td>14</td>\n",
       "      <td>471</td>\n",
       "      <td>84</td>\n",
       "      <td>144</td>\n",
       "      <td>62</td>\n",
       "      <td>167</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Post</th>\n",
       "      <td>244</td>\n",
       "      <td>804</td>\n",
       "      <td>318</td>\n",
       "      <td>672</td>\n",
       "      <td>3061</td>\n",
       "      <td>820</td>\n",
       "      <td>33</td>\n",
       "      <td>468</td>\n",
       "      <td>828</td>\n",
       "      <td>269</td>\n",
       "      <td>137</td>\n",
       "      <td>292</td>\n",
       "      <td>818</td>\n",
       "      <td>585</td>\n",
       "      <td>2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York Times</th>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "      <td>66</td>\n",
       "      <td>148</td>\n",
       "      <td>513</td>\n",
       "      <td>16</td>\n",
       "      <td>1907</td>\n",
       "      <td>83</td>\n",
       "      <td>131</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>115</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reuters</th>\n",
       "      <td>171</td>\n",
       "      <td>531</td>\n",
       "      <td>110</td>\n",
       "      <td>152</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>335</td>\n",
       "      <td>659</td>\n",
       "      <td>334</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>142</td>\n",
       "      <td>991</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talking Points Memo</th>\n",
       "      <td>202</td>\n",
       "      <td>1281</td>\n",
       "      <td>234</td>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>181</td>\n",
       "      <td>25</td>\n",
       "      <td>552</td>\n",
       "      <td>172</td>\n",
       "      <td>41</td>\n",
       "      <td>112</td>\n",
       "      <td>58</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vox</th>\n",
       "      <td>143</td>\n",
       "      <td>694</td>\n",
       "      <td>221</td>\n",
       "      <td>41</td>\n",
       "      <td>827</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>84</td>\n",
       "      <td>271</td>\n",
       "      <td>267</td>\n",
       "      <td>167</td>\n",
       "      <td>78</td>\n",
       "      <td>160</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington Post</th>\n",
       "      <td>400</td>\n",
       "      <td>1876</td>\n",
       "      <td>479</td>\n",
       "      <td>202</td>\n",
       "      <td>747</td>\n",
       "      <td>102</td>\n",
       "      <td>151</td>\n",
       "      <td>336</td>\n",
       "      <td>169</td>\n",
       "      <td>622</td>\n",
       "      <td>182</td>\n",
       "      <td>154</td>\n",
       "      <td>378</td>\n",
       "      <td>198</td>\n",
       "      <td>1238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                  0     1     2    3     4     5     6    7    8     9   \\\n",
       "publication                                                                    \n",
       "Atlantic              203   863   235  159  1183   204     9  163  131   333   \n",
       "Breitbart            1043  3193  1968  429  1432   146   276  431  296  1375   \n",
       "Business Insider      153   874   258  114   883   150    10   84  696   244   \n",
       "Buzzfeed News          93   402   105  118   391    36     9  216  325   176   \n",
       "CNN                   420  1260   399  326  1202   264    18  315  111   581   \n",
       "Fox News              124   491   510  117   201    68    20  101   38   195   \n",
       "Guardian              154   800   122  300  1405   214    28  192  198   311   \n",
       "NPR                   197   744   252  233  1494  2267    19  255  180   347   \n",
       "National Review       400   879   377   46   604    69    10  131   14   471   \n",
       "New York Post         244   804   318  672  3061   820    33  468  828   269   \n",
       "New York Times         32    49    66  148   513    16  1907   83  131    80   \n",
       "Reuters               171   531   110  152    63    13     2  335  659   334   \n",
       "Talking Points Memo   202  1281   234   48   200    43    12  181   25   552   \n",
       "Vox                   143   694   221   41   827    66    11   88   84   271   \n",
       "Washington Post       400  1876   479  202   747   102   151  336  169   622   \n",
       "\n",
       "col_0                 10   11    12   13    14  \n",
       "publication                                     \n",
       "Atlantic             102  222   153  161   742  \n",
       "Breitbart            307  455  1093  517  4685  \n",
       "Business Insider      59   57   143   35  1145  \n",
       "Buzzfeed News        106  136   318    1   951  \n",
       "CNN                  216  221   824    6  2036  \n",
       "Fox News              44   35   351   87   732  \n",
       "Guardian              88  230   337    0  1012  \n",
       "NPR                  330  276   276  160   997  \n",
       "National Review       84  144    62  167   541  \n",
       "New York Post        137  292   818  585  2245  \n",
       "New York Times        86  115    96   95   420  \n",
       "Reuters               57   34   142  991   865  \n",
       "Talking Points Memo  172   41   112   58   634  \n",
       "Vox                  267  167    78  160   353  \n",
       "Washington Post      182  154   378  198  1238  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=15, init='k-means++', random_state=42, n_init=20)\n",
    "y_pred = kmeans.fit_predict(X_normal)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93917, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.03282533\n",
      "Silhouette Score: 0.09494151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_normal.values, y_pred, sample_size=6000, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f7a32311e0ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_normal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\spectral.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    461\u001b[0m             self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n\u001b[0;32m    462\u001b[0m                                                      \u001b[0mfilter_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m                                                      **params)\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[1;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown kernel %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[1;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m     \u001b[0mK\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# exponentiate K in-place\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc = SpectralClustering(n_clusters=15)\n",
    "y_pred3 = sc.fit_predict(X_normal)\n",
    "\n",
    "pd.crosstab(y_train, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred3)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred3, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = Affinity Propagation\n",
    "y_pred4 = af.fit_predict(X2_train)\n",
    "\n",
    "pd.crosstab(y2_train, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y2_train, y_pred4)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X2_train, y_pred4, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rfc = ensemble.RandomForestClassifier()\n",
    "train = tf_rfc.fit(X_normal, y_train)\n",
    "\n",
    "print('Training set score:', tf_rfc.score(X_normal, y_train))\n",
    "print('\\nTest set score:', tf_rfc.score(X_normal, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lr = LogisticRegression()\n",
    "train = tf_lr.fit(X_normal, y_train)\n",
    "\n",
    "print('Training set score:', tf_lr.score(X_normal, y_train))\n",
    "print('\\nTest set score:', tf_lr.score(X_normal, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
